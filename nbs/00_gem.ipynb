{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gem\n",
    "\n",
    "> Simple utilities for working with Google's Gemini API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a minimal interface to Google's Gemini API. The goal is to make it dead simple to:\n",
    "\n",
    "1. Generate text with just a prompt\n",
    "2. Analyze files (PDFs, images) \n",
    "3. Process videos (like YouTube)\n",
    "\n",
    "All through a single `gem()` function that just works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp gem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, make sure you have your Gemini API key set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from pathlib import Path\n",
    "from fastcore.all import *\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export GEMINI_API_KEY='your-api-key'\n",
    "assert os.environ.get(\"GEMINI_API_KEY\"), \"Please set GEMINI_API_KEY environment variable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building blocks\n",
    "\n",
    "Let's start with the simple helper functions that make everything work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client creation\n",
    "\n",
    "We need a Gemini client to talk to the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _client():\n",
    "    \"Get Gemini client\"\n",
    "    return genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "c = _client()\n",
    "assert c is not None\n",
    "assert hasattr(c, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting attachments to Parts\n",
    "\n",
    "Gemini expects different types of content (files, URLs) to be wrapped in \"Parts\". This helper handles that conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _is_url(s):\n",
    "    \"Check if string is a URL\"\n",
    "    if not isinstance(s, str): return False\n",
    "    return (s.startswith('http://') or \n",
    "            s.startswith('https://') or \n",
    "            s.startswith('www.') or \n",
    "            'youtube.com' in s or \n",
    "            'youtu.be' in s)\n",
    "\n",
    "def _make_part(o):\n",
    "    \"Convert object to Gemini Part\"\n",
    "    if isinstance(o, (str, Path)):\n",
    "        p = Path(o)\n",
    "        if p.exists():\n",
    "            mime_map = {'.pdf': 'application/pdf', \n",
    "                        '.png': 'image/png', \n",
    "                        '.jpg': 'image/jpeg', \n",
    "                        '.jpeg': 'image/jpeg', \n",
    "                        '.gif': 'image/gif'}\n",
    "            mime = mime_map.get(p.suffix.lower(), 'application/octet-stream')\n",
    "            return types.Part.from_bytes(mime_type=mime, data=p.read_bytes())\n",
    "        elif _is_url(o): return types.Part.from_uri(file_uri=o, mime_type='video/*')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main interface\n",
    "\n",
    "Now we can build our main `gem()` function that handles all use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def gem(prompt, # Text prompt\n",
    "        o=None, # Optional file/URL attachment or list of attachments\n",
    "        model='gemini-2.5-flash',\n",
    "        thinking=-1,\n",
    "        search=False):\n",
    "    \"Generate content with Gemini\"\n",
    "    parts = [types.Part.from_text(text=prompt)]\n",
    "    \n",
    "    # Handle single attachment or list of attachments\n",
    "    attachments = o if isinstance(o, list) else [o] if o else []\n",
    "    for attachment in attachments:\n",
    "        if part := _make_part(attachment): parts.insert(0, part)\n",
    "    \n",
    "    contents = types.Content(role='user', parts=parts) if attachments else prompt    \n",
    "    config_dict = {\n",
    "        'thinking_config': types.ThinkingConfig(thinking_budget=thinking),\n",
    "        'response_mime_type': 'text/plain'\n",
    "    }\n",
    "    # Adjust media_resolution for videos for more tokens\n",
    "    if any(attachment and not Path(str(attachment)).exists() for attachment in attachments): \n",
    "        config_dict['media_resolution'] = 'MEDIA_RESOLUTION_LOW'\n",
    "    config_dict['tools'] = []\n",
    "    if search: config_dict['tools'].append(types.Tool(google_search=types.GoogleSearch()))\n",
    "    cfg = types.GenerateContentConfig(**config_dict)\n",
    "    resp = _client().models.generate_content(model=model, contents=contents, config=cfg)\n",
    "    return resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "One function handles everything:\n",
    "- Just text? Pass a prompt.\n",
    "- Have a file? Pass it as the second argument.\n",
    "- Got a YouTube URL? Same thing.\n",
    "\n",
    "Let's test it out:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation\n",
    "\n",
    "The simplest case - just generate some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean, clear code unfolds,\\nIndents guide the powerful flow,\\nProblems solved with ease.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gem(\"Write a haiku about Python programming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video analysis\n",
    "\n",
    "Perfect for creating YouTube chapters or summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Late interaction beats single vector.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"5 word summary of this video.\"\n",
    "gem(prompt, \"https://youtu.be/1x3k0V2IITo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File analysis\n",
    "\n",
    "Great for extracting information from PDFs or images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This presentation introduces new frontiers in Information Retrieval (IR), focusing on instruction following and reasoning capabilities, much like Large Language Models (LLMs). It presents two key models: Promptriever, a fast bi-encoder trained to follow natural language instructions for retrieval, and Rank1, a slower cross-encoder capable of complex, test-time reasoning for judging document relevance. These \"promptable\" and \"reasoning\" retrievers significantly enhance search performance, unlock new types of queries, and can even uncover previously overlooked relevant documents.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gem(\"3 sentence summary of this presentation.\", \"NewFrontiersInIR.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This image is a digital graphic, likely a thumbnail for a video or presentation, set against a dark blue or black background.\\n\\nHere\\'s a detailed breakdown of its contents:\\n\\n1.  **Person:** On the lower-left side, a young Caucasian man with short brown hair is smiling broadly, showing his teeth. He is wearing a light-colored (possibly white) V-neck shirt, with his head and upper chest visible.\\n2.  **Emoji:** Above and slightly to the left of the man\\'s head, there is a yellow \"sad face\" emoji with downturned eyes and mouth.\\n3.  **Text:**\\n    *   In the upper-left, white text reads: \"Single Vector?\"\\n    *   Below this, and slightly to the right of the emoji, large, bold yellow text is stacked vertically:\\n        *   \"YOU\\'RE\"\\n        *   \"MISSING\"\\n        *   \"OUT\"\\n    *   Combined, the main text reads: \"Single Vector? YOU\\'RE MISSING OUT\"\\n4.  **Diagram/Network:** On the right side, there\\'s an abstract glowing blue diagram resembling a neural network or a data flow graph. It features multiple interconnected blue circular nodes (or \"neurons\") linked by glowing blue lines.\\n    *   There appears to be an upper cluster of nodes and a lower cluster.\\n    *   An arrow points upwards from the lower cluster towards the upper cluster.\\n    *   Another arrow points downwards from the upper cluster towards a dark blue rectangular box with rounded corners.\\n5.  **\"RAG\" Box:** Inside the blue rectangular box on the lower-right, the white, bold capital letters \"RAG\" are prominently displayed.\\n\\nThe overall impression is that of a promotional image, possibly related to technology, artificial intelligence, or data processing, with the text suggesting a warning or a missed opportunity for those who stick to \"single vector\" approaches, implying that the RAG (Retrieval-Augmented Generation) concept shown in the diagram is a superior method.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gem(\"What's in this image?\", \"anton.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Model\n",
    "\n",
    "You can also control the model and thinking time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on his public profiles and professional presence, Hamel Husain's current job is **Co-founder and CEO** of **Gantry**.\\n\\nGantry is a company that provides an AI observability platform designed to help teams monitor, analyze, and improve their machine learning models in production.\\n\\nBefore co-founding Gantry in 2022, he was well-known for his role as the Head of Machine Learning at GitHub.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gem(\"What is Hamel Husain's current job?\", model=\"gemini-2.5-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grounded Search\n",
    "\n",
    "As you can see, grounded search is required to get things right sometimes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nthought\\nThe search results indicate that Hamel Husain is currently an independent consultant specializing in AI and machine learning, particularly in operationalizing Large Language Models (LLMs). While he was previously a Staff Machine Learning Engineer at GitHub and is still listed as such in some contexts, more recent information from March and July 2025 explicitly states his role as an independent consultant. This suggests his independent consulting is his current primary job. Therefore, I have sufficient information to answer the user's request.Hamel Husain is currently an independent consultant, specializing in helping companies build, evaluate, and operationalize AI-powered systems and Large Language Models (LLMs). He focuses on making AI more reliable, understandable, and actionable.\\n\\nPreviously, he held the position of Staff Machine Learning Engineer at GitHub, where he was involved in the design and development of software engineering, machine learning, and developer tools, and led engineering teams. His work at GitHub included leading research efforts on projects like CodeSearchNet, which was a precursor to GitHub Copilot. He has also worked at companies such as Airbnb, DataRobot, and Accenture.Hamel Husain is currently an independent consultant, running Parlance Labs, where he helps companies build AI products and operationalize Large Language Models (LLMs). He also serves on the R&D team at AnswerAI and scouts for Bain Capital.\\n\\nPreviously, he was a Staff Machine Learning Engineer at GitHub, where he led research efforts on projects such as CodeSearchNet, a precursor to GitHub Copilot. His extensive experience in machine learning spans over 20 years, with past roles at companies like Airbnb, DataRobot, and Accenture.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gem(\"What is Hamel Husain's current job?.\", search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Attachments\n",
    "\n",
    "You can analyze multiple files/URLs at once by passing a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Is this PDF and YouTube video related or are they different talks? Answer with very short yes/no answer.\"\n",
    "gem(prompt, [\"https://youtu.be/Trps2swgeOg?si=yK7CO0Zk4E1rfp6s\", \"NewFrontiersInIR.pdf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gem(prompt, [\"https://youtu.be/YB3b-wPbSH8?si=WI0LqflY5SYIsRz9\", \"NewFrontiersInIR.pdf\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcuts\n",
    "\n",
    "### Functions to help you do common tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def yt_chapters(link):\n",
    "    \"Generate YoutTube Summary and Chapters From A Public Video.\"\n",
    "    \n",
    "    chapter_prompt=\"Generate a succinct video summary (1-2 sentences) followed by YouTube chapter timestamps for this video. Format each line of the chapter summaries as 'MM:SS - Chapter Title' (e.g., '02:30 - Introduction'). Start with 00:00. Include all major topics and transitions and be thorough - do not miss any important topics.  For the summary, do not say 'In this video, we will cover the following topics', 'This video discusses..' or anything like that. Instead, reference the main speaker's name if you know it.  If there is a Q&A Section, enumerate individual questions as additional chapters.\"\n",
    "    return gem(prompt=chapter_prompt, o=link, model=\"gemini-2.5-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what it looks like for Antoine's [Late Interaction Talk](https://youtu.be/1x3k0V2IITo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antoine Chaffin explains the inherent limitations of single vector search, such as information loss from pooling and poor performance in out-of-domain and long-context scenarios. He then introduces late interaction (multi-vector) models as a superior alternative that avoids these pitfalls and presents the PyLate library to make training and evaluating these powerful models more accessible.\n",
      "\n",
      "00:00 - Going Further: Late Interaction Beats Single Vector Limits\n",
      "00:32 - About Antoine Chaffin\n",
      "01:40 - Explaining Dense (Single) Vector Search\n",
      "03:08 - Why Single Vector Search is the Go-To for RAG\n",
      "03:54 - Performance Evaluation and the MTEB Leaderboard\n",
      "04:17 - BEIR: A School Case of Goodhart's Law\n",
      "05:36 - Limitations Beyond Standard Benchmarks\n",
      "08:24 - Pooling: The Intrinsic Flaw of Dense Models\n",
      "08:41 - How Pooling Creates Problems in Production\n",
      "10:42 - The Advantage of BM25\n",
      "11:32 - Replacing Pooling with Late Interaction\n",
      "12:17 - Why Not Just Use a Bigger Single Vector?\n",
      "13:51 - Performance Comparison: Late Interaction vs. Dense Models\n",
      "16:48 - Interpretability: A Nice Little Bonus\n",
      "17:42 - Why Are People Still Using Dense Models?\n",
      "18:43 - PyLate: Extending Sentence Transformers for Multi-Vector Models\n",
      "21:28 - Evaluating Models with PyLate\n",
      "22:49 - Future Avenues for Research\n",
      "24:36 - Conclusion and Resources\n",
      "25:51 - Q&A: Latency of Late Interaction vs. Dense Vector\n",
      "31:00 - Q&A: Fine-tuning Comparison\n",
      "33:20 - Q&A: Tips for Fine-tuning with PyLate\n"
     ]
    }
   ],
   "source": [
    "chp = yt_chapters(\"https://youtu.be/1x3k0V2IITo\")\n",
    "print(chp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
