{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17ba50a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from fastcore.utils import Path\n",
    "ftxt = types.Part.from_text\n",
    "fbyt = types.Part.from_bytes\n",
    "tcfg = types.ThinkingConfig\n",
    "gcfg = types.GenerateContentConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4821822",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2d78ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outline(pdf_path, \n",
    "                prompt=\"Provide a numbered list of each slide with a one sentence summary of each.  The numbered list should correspond uniquely with each page of the attached pdf.\",\n",
    "                model=\"gemini-2.5-pro\"):\n",
    "    \n",
    "    contents = [types.Content(role=\"user\",\n",
    "                parts=[ftxt(text=prompt),\n",
    "                       fbyt(mime_type=\"application/pdf\", data=Path(pdf_path).read_bytes())],\n",
    "            ),\n",
    "           ]\n",
    "    cfg = gcfg(thinking_config = tcfg(thinking_budget=-1), response_mime_type=\"text/plain\")\n",
    "    return client.models.generate_content(model=model, contents=contents, config=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7181af9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a numbered list of one-sentence summaries for each slide of the presentation.\n",
      "\n",
      "1.  This is the title slide for a presentation by Orion Weller from Johns Hopkins University on \"New Frontiers in IR: Instruction Following and Reasoning.\"\n",
      "2.  This slide illustrates the integration of web search functionality directly within a conversational AI interface like ChatGPT.\n",
      "3.  The concept of \"Instruction Following\" is demonstrated with an example of an AI successfully generating a haiku that adheres to multiple specific constraints.\n",
      "4.  This slide exemplifies \"Reasoning\" by showing an AI's internal thought process as it correctly counts the number of 'r's in the word \"strawberry.\"\n",
      "5.  This slide poses the central question of how the advanced capabilities of instruction following and reasoning can be applied to information retrieval.\n",
      "6.  A screenshot of the original Google Beta homepage from 1999 is shown to represent the starting point of traditional keyword-based web search.\n",
      "7.  The modern Google search interface is displayed to show that while the design has evolved, the core functionality remains based on keyword queries.\n",
      "8.  A mock-up of \"SearchGPT\" illustrates a more advanced search paradigm that provides synthesized, direct answers instead of just a list of links.\n",
      "9.  This slide argues that despite the integration of LLMs, search has not fundamentally changed, as they are often just a \"wrapper\" around traditional search results.\n",
      "10. The concept of \"Keyword Search\" is introduced with a simple query example for finding websites about data privacy.\n",
      "11. This slide demonstrates a limitation of keyword search, as it incorrectly flags a thematically irrelevant document while marking two others as relevant.\n",
      "12. \"Semantic Search\" is presented as an improvement that correctly understands the meaning of the query and identifies all three documents as thematically relevant.\n",
      "13. \"Instruction-based Search\" introduces a query with a complex stylistic constraint that current systems, including LLM-based rerankers, cannot solve.\n",
      "14. This slide shows the desired outcome of instruction-based search, which correctly identifies the single document that meets all the query's constraints.\n",
      "15. The concept is advanced to \"Prompt and Reasoning-based Search,\" where the query includes nuanced user intent and preferences, such as the need for high recall.\n",
      "16. This slide defines an \"instruction\" in IR as a combination of document attributes, NLU aspects like sentiment and style, and logical conditions.\n",
      "17. The presenter proposes that information retrievers should be used in the same prompt-based way that we now use language models.\n",
      "18. Two new systems are introduced: \"Promptriever,\" a fast embedder, and \"Rank1,\" a strong but slow reranker.\n",
      "19. This is a transition slide that highlights \"Promptriever\" as the first topic of discussion.\n",
      "20. This is the title slide for \"Promptriever,\" a system described as an instruction-trained retriever that can be prompted like a language model.\n",
      "21. This slide diagrams and contrasts the architectures of a fast Bi-Encoder and a powerful but slow Cross-Encoder, which are foundational to modern retrieval systems.\n",
      "22. The key research question is posed: can we create promptable bi-encoder retrievers by providing them with the right instruction-following training data?\n",
      "23. The process of generating training data is introduced, starting with an example query from an existing dataset.\n",
      "24. The slide shows how a query is paired with a known positive document to form the basis of a training example.\n",
      "25. To enable instruction following, synthetic instructions are generated to explicitly describe the relationship between the original query and the positive document.\n",
      "26. The experimental settings are detailed, starting from the RepLLaMA training recipe and evaluating on in-domain, instruction-based, and out-of-domain datasets.\n",
      "27. This slide serves as a simple title card for the \"Evaluation Data\" section.\n",
      "28. The FollowIR dataset is introduced as a benchmark for evaluating complex instruction-following capabilities in information retrieval.\n",
      "29. The InstructIR dataset is described, which uses 10 different user personas per query to test how well a model can adapt to varied intents.\n",
      "30. This slide serves as a simple title card for the \"Results\" section.\n",
      "31. An empty chart is presented, setting up a comparison between the \"RepLLaMA\" and \"Promptriever\" models on two instruction-following benchmarks.\n",
      "32. The first results show that Promptriever significantly outperforms RepLLaMA on the FollowIR benchmark for complex instructions.\n",
      "33. The completed chart demonstrates that Promptriever also achieves a much higher score than RepLLaMA on the persona-based InstructIR benchmark.\n",
      "34. This is a blank transition slide titled \"Results.\"\n",
      "35. The slide explains the methodology for testing on standard datasets without explicit instructions, which involves using both no prompt and the best of 10 generic prompts.\n",
      "36. Results on the out-of-domain BEIR benchmark show that without a prompt, Promptriever performs on par with the baseline RepLLaMA model.\n",
      "37. When a generic prompt is added, Promptriever's performance on the out-of-domain BEIR benchmark significantly improves, surpassing the baseline.\n",
      "38. An empty box plot is introduced to compare the performance stability of different models across 10 different generic prompts.\n",
      "39. The results show that Promptriever is significantly more robust and less sensitive to prompt wording than the baseline models, as indicated by its much lower standard deviation.\n",
      "40. This slide summarizes that with the right data, retrievers can be made promptable like LMs, unlocking new query types and enabling natural language instructions.\n",
      "41. This is a transition slide that shifts focus from the fast \"Promptriever\" to the \"strong but slow\" Rank1 system.\n",
      "42. This is an animated transition slide that highlights \"Rank1\" as the second topic of the presentation.\n",
      "43. This is the title slide for \"Rank1,\" a system that uses test-time compute to improve information retrieval performance.\n",
      "44. The diagram comparing Bi-Encoder and Cross-Encoder architectures is shown again to provide context for reranking models.\n",
      "45. The concept of \"Test-Time Compute\" is illustrated by a graph showing that a model's accuracy on a reasoning task increases as it is given more computation time.\n",
      "46. This slide poses the question of what test-time compute looks like in the context of an information retrieval task.\n",
      "47. A simple IR example is given, pairing a user's question with a potentially relevant document snippet.\n",
      "48. This slide demonstrates how a model can use a chain-of-thought process at test-time to reason about the nuances of a query and document, leading to a more accurate relevance judgment.\n",
      "49. This slide serves as a simple title card for the \"Evaluation Data\" section.\n",
      "50. The BRIGHT dataset is introduced, which is designed to test a model's reasoning ability by requiring it to identify abstract relationships between queries and documents.\n",
      "51. An example of model reasoning is shown, where the system correctly identifies that a query and a document are relevant because they share a similar algorithmic approach.\n",
      "52. This slide serves as a simple title card for the \"Results\" section.\n",
      "53. The evaluation setup is presented, comparing Rank1 against RankLLaMA on tasks requiring reasoning, negation, and instruction-following, noting RankLLaMA used 10x more training data.\n",
      "54. On the BRIGHT reasoning dataset, results show that Rank1 nearly doubles the performance of the baseline RankLLaMA model.\n",
      "55. On the NevIR negation task, Rank1 once again dramatically outperforms RankLLaMA, more than doubling its accuracy.\n",
      "56. On the mFollowIR instruction-following task, results show Rank1 is more than twice as effective as the baseline RankLLaMA model.\n",
      "57. This slide questions the direct impact of the reasoning chain by proposing a direct comparison between the same model with and without it.\n",
      "58. A bar chart confirms that using a reasoning chain significantly boosts the model's performance on the BRIGHT dataset compared to not using one.\n",
      "59. This slide transitions the presentation to discuss performance on older, traditional evaluation datasets.\n",
      "60. Results on the older DL19/DL20 datasets show surprisingly low scores for modern rerankers when evaluated only against existing relevance judgments.\n",
      "61. After re-judging the results, all models' scores improve, with Rank1 achieving the highest score by finding new, relevant documents that previous systems missed.\n",
      "62. This summary concludes that using test-time compute creates powerful reasoning rerankers that, while slower, are much more effective than previous approaches.\n",
      "63. This transition slide contrasts the two presented systems, recapping Promptriever as the \"fast embedder\" and Rank1 as the \"strong but slow\" reranker.\n",
      "64. The presentation's ultimate goal is restated: to create information retrieval systems that can handle complex, instruction-based queries just like LLMs.\n",
      "65. The final slide concludes that new retrievers can benefit from LLM advancements to enable true instruction-based search, while providing contact info and noting the models are open-source.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "response = get_outline(\"NewFrontiersInIR.pdf\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff00a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
