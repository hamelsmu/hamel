[
  {
    "objectID": "writing.html",
    "href": "writing.html",
    "title": "Writing Utils",
    "section": "",
    "text": "Split PDF files into individual slide images. Requires poppler-utils installed (brew install poppler on macOS or apt-get install poppler-utils on Ubuntu).\n\nsource\n\n\n\n pdf2imgs (pdf_path, output_dir='.', prefix='slide')\n\nSplit a PDF file into individual slide images using poppler’s pdftoppm.\nFor example, you can split the NewFrontiersInIR.pdf file into individual slide images:\n\n# Split NewFrontiersInIR.pdf into individual slides\noutput_folder = \"slides_output\"\nimage_files = pdf2imgs(\"NewFrontiersInIR.pdf\", output_dir=output_folder)\n\n# Show number of slides created\nprint(f\"Created {len(image_files)} slide images in {output_folder}/\")\n\nCreated 65 slide images in slides_output/\n\n\n\n!rm -rf slides_output/",
    "crumbs": [
      "Writing Utils"
    ]
  },
  {
    "objectID": "writing.html#pdf-to-images",
    "href": "writing.html#pdf-to-images",
    "title": "Writing Utils",
    "section": "",
    "text": "Split PDF files into individual slide images. Requires poppler-utils installed (brew install poppler on macOS or apt-get install poppler-utils on Ubuntu).\n\nsource\n\n\n\n pdf2imgs (pdf_path, output_dir='.', prefix='slide')\n\nSplit a PDF file into individual slide images using poppler’s pdftoppm.\nFor example, you can split the NewFrontiersInIR.pdf file into individual slide images:\n\n# Split NewFrontiersInIR.pdf into individual slides\noutput_folder = \"slides_output\"\nimage_files = pdf2imgs(\"NewFrontiersInIR.pdf\", output_dir=output_folder)\n\n# Show number of slides created\nprint(f\"Created {len(image_files)} slide images in {output_folder}/\")\n\nCreated 65 slide images in slides_output/\n\n\n\n!rm -rf slides_output/",
    "crumbs": [
      "Writing Utils"
    ]
  },
  {
    "objectID": "writing.html#gather-context-from-webpages",
    "href": "writing.html#gather-context-from-webpages",
    "title": "Writing Utils",
    "section": "Gather Context From Webpages",
    "text": "Gather Context From Webpages\nI often want to gather context from a set of web pages.\n\nsource\n\ngather_urls\n\n gather_urls (urls, tag='example')\n\nGather contents from URLs.\n\nsource\n\n\njina_get\n\n jina_get (url)\n\nGet a website as md with Jina.\nFor example, these are what I might use as context for annotated posts\n\n_annotated_post_content = gather_urls(_annotated_post_urls)\nprint(_annotated_post_content[:500])\n\n&lt;examples&gt;\n&lt;example-1&gt;\nTitle: \n\nURL Source: https://raw.githubusercontent.com/hamelsmu/hamel-site/refs/heads/master/notes/llm/rag/p1-intro.md\n\nMarkdown Content:\n---\ntitle: \"P1: I don't use RAG, I just retrieve documents\"\ndescription: \"Ben Clavié's introduction to advanced retrieval techniques\"\nimage: p1-images/slide_12.png\ndate: 2025-06-25\n---\n\nAs part of our [LLM Evals course](https://bit.ly/evals-ai){target=\"_blank\"}, I hosted [Benjamin Clavié](https://ben.clavie.eu/){target=\"_blank\"} to kick \n\n\n\nsource\n\n\noutline_slides\n\n outline_slides (slide_path)\n\n\n_o = outline_slides('NewFrontiersInIR.pdf')\nprint(_o[:300])\n\nHere is a one-sentence summary for each slide:\n\n1.  This slide introduces the presentation \"New Frontiers in IR: Instruction Following and Reasoning\" by Orion Weller from Johns Hopkins Whiting School of Engineering.\n2.  This slide shows a \"Message ChatGPT\" interface with a prominent \"Search\" button,",
    "crumbs": [
      "Writing Utils"
    ]
  },
  {
    "objectID": "writing.html#annotated-posts-from-talk",
    "href": "writing.html#annotated-posts-from-talk",
    "title": "Writing Utils",
    "section": "Annotated Posts From Talk",
    "text": "Annotated Posts From Talk\n\nsource\n\ngenerate_annotated_talk_post\n\n generate_annotated_talk_post (slide_path, youtube_link, image_dir,\n                               transcript_path=None, example_urls=['https:\n                               //raw.githubusercontent.com/hamelsmu/hamel-\n                               site/refs/heads/master/notes/llm/rag/p1-\n                               intro.md', 'https://raw.githubusercontent.c\n                               om/hamelsmu/hamel-site/refs/heads/master/no\n                               tes/llm/rag/p2-evals.md', 'https://raw.gith\n                               ubusercontent.com/hamelsmu/hamel-site/refs/\n                               heads/master/notes/llm/evals/inspect.qmd'])\n\nAssemble the prompt for the annotated post.",
    "crumbs": [
      "Writing Utils"
    ]
  },
  {
    "objectID": "writing.html#example-post",
    "href": "writing.html#example-post",
    "title": "Writing Utils",
    "section": "Example Post",
    "text": "Example Post\n\npost = generate_annotated_talk_post(slide_path='orion_example/NewFrontiersInIR.pdf',\n                                    youtube_link='https://youtu.be/YB3b-wPbSH8?si=u_x0Puwreld3YCGf',\n                                    image_dir='orion_example/p3_images',\n                                    transcript_path='orion_example/transcript.md')\n\n\nPath('orion_example/p3_orion.qmd').write_text(post)\n\n30263",
    "crumbs": [
      "Writing Utils"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hamel",
    "section": "",
    "text": "A collection of personal utilities for various tasks.",
    "crumbs": [
      "hamel"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "hamel",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/hamelsmu/hamel.git\nor from pypi\n$ pip install hamel\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages.",
    "crumbs": [
      "hamel"
    ]
  },
  {
    "objectID": "index.html#available-utilities",
    "href": "index.html#available-utilities",
    "title": "hamel",
    "section": "Available Utilities",
    "text": "Available Utilities\n\nGemini API (hamel.gem)\nSimple interface for Google’s Gemini API. See the gem module documentation for details.",
    "crumbs": [
      "hamel"
    ]
  },
  {
    "objectID": "orion_example/transcript.html",
    "href": "orion_example/transcript.html",
    "title": "Reasoning Opens up New Retrieval Frontiers",
    "section": "",
    "text": "[00:00:00]\n[00:00:00] Orion Weller: Today, I’ll be talking about new frontiers in retrieval or information retrieval ir, about instruction following and reasoning. If you are using language models today, like you’re [00:00:10] talking the chat, GPT there’s a couple things that have been huge for defining them and helping them be really useful for users.\n\n\n\n[00:00:18] Orion Weller: Two of the biggest ones I think [00:00:20] are instruction following. and if you’ve used a language model before, you can, it does all sorts of things. You can just tell it what you want and it will go out and do it. So here I ask [00:00:30] chat, GBT to generate a haiku about information retrieval in the style of a pirate and to mention rag. And it can follow my instructions perfectly.\nSo here’s this nice little haiku. [00:00:40] It mentions rag, it talks like a pirate. And this actually I feel like may be underwhelming to us today because we just expect them to do this, right? They’re [00:00:50] so good at instruction following, you can just tell a language model what you want, and it’ll give you what you want. But a few years ago, this was not the case.\n\n\n\n[00:00:58] Orion Weller: The other is reasoning, and this is something [00:01:00] that’s really hot in the community today. Pretty much any major player in language model space will have some reasoning model. And what sets these apart is that when you [00:01:10] ask it a question or type something in, it’s going to give these thinking tokens. So here’s one from Opening Eyes oh one, and here it thinks about how many Rs are in the word [00:01:20] strawberry.\nIt’ll generate these sort of intermediate tokens. And then finally it’ll end by telling you the final output after it thinks. so this is known as [00:01:30] reasoning, thinking, sometimes test time, compute all these sorts of things. And so reasoning and instruction following are two of the most major things that are making language [00:01:40] models so useful these days.\n\n\n\n[00:01:41] Orion Weller: What I wanna talk about today is how can we use these in retrieval? It’s like, how does this relate? have these really amazing language models. How can we bring them [00:01:50] over? If you look at Google in 1999, it looks like this, which is really actually quite similar to how [00:02:00] Google search looks today in 2025, So we have 26 years of development, it’s still very much in that you are gonna type words into a [00:02:10] box. It’s gonna go out and do some form of matching. Then it’s gonna come back and give you your list of websites. We do have nice new things like search GPT [00:02:20] these days. So if you see this, it’s a very nice ui. But what happens behind the scenes is it’s going to send your query to a search engine. In fact, often like Bing or Google [00:02:30] itself, just normal search, and then it’s gonna pass those results back to the language model, which then creates this very nice output. So [00:02:40] search itself really hasn’t changed very much. Search, GPT is still using a normal search engine. despite the fact that these days we’re using language [00:02:50] models, we’re training models even from Llama to be for retrieval, they still work pretty much the same. So we’re mostly just adding these [00:03:00] wrappers around the results of search. So what I wanna talk about today is pushing past this. Can we make retrieval use all the capabilities of the language model? How [00:03:10] can we unlock all these capabilities that we see that are going on in language models and bring them into retrieval? Lemme show some examples.\n\n\n\n[00:03:23] Orion Weller: back in [00:03:20] 1999, they mostly worked on search. This was before semantic search and mostly was just keyword matching, also known as lexical matching. [00:03:30] So you might have a query, like find websites explaining data privacy. You might also have some collection of documents. This could be things like data encryption [00:03:40] standards from nist, a government website, maybe a blog post called The Wolves Outside Your Data. And then another document called digital protection from a think [00:03:50] tank called Clear Law. And so if you were gonna pass these to a search engine from 1999, they used exact keyword search. You’re probably going to find the [00:04:00] first two documents, but not the third. And that’s because the third one here is like a synonym. It’s data and digital privacy and protection. So it wouldn’t match exactly. [00:04:10] However, we have come a long way in 25 years. So now we have semantic search, here what we do is we match it in semantic space. So [00:04:20] you’re going to go out and you’re gonna look for things like paraphrases and in the same similar space there. A good semantic search engine shouldn’t find all three of these. [00:04:30] So the next step I think is where we should be going with retrieval and where I feel like this new paradigm will really unlock a lot of great benefits.\n\n\n\n[00:04:38] Orion Weller: Lemme start off by talking about [00:04:40] instruction based search. imagine you have the same query, but you’re gonna add, you want a website that uses an extended metaphor to explain. [00:04:50] Here, there should only be one document that’s relevant, the one’s about wolves outside your data. But note here, you can’t do any form of keyword matching to find this document, [00:05:00] right? It’s not gonna say the word metaphor. It probably won’t even say a synonym of the word metaphor, like allegory or something like that. really have to understand the whole [00:05:10] document itself to be able to do this reasoning, to see that, oh, this is using a metaphor. So this is where I think instruction based search opens up these new categories of retrieval that you couldn’t [00:05:20] get before, any sort of meta document reasoning, But just to show how far you could push this I think you could move on. Oh, before I do that it’s important to note here that you can’t [00:05:30] solve this simply by using language models to reran. if you have a big collection, say a million documents. wanna find one with an extended metaphor. [00:05:40] If you just take the top a hundred or a thousand that could fit in a language model, you may not find them right because you’re not gonna be able to keyword match metaphor. [00:05:50] So I think this is really important that we still need these sort of embedding retrieval models to be able to find these because this is not something we could just entirely throw to a language model. There’s just so much [00:06:00] out there. So then the next one I think is out there, but I think really demonstrates how far you could push this, right?\n\n\n\n[00:06:11] Orion Weller: And I’m gonna call this prompting based search or [00:06:10] reasoning based search. Here you could add on things like, I need you to have really high recall, or I’m going to lose my job. And so imagine you were just typing this into Google search, right? You’re gonna [00:06:20] type into Google search. I need you to have a really high recall. It’s definitely going to try to look for the word recall, It’s gonna try to keyword match that. again, we want [00:06:30] the model, which is a language model, to understand the whole intent of the query and be able to go out and find it.\nAnd this time it’ll think, oh, I need to have really good recall. I need to like, [00:06:40] be really careful here and hopefully it does better. Some areas I think it could help the neuro like document attributes. So these days we pre-process things. You add attributes like date, length, [00:06:50] source, all these sorts of things we have to manually add to documents. But an instruction based retriever should just be able to look at it and know, because it understands these things.\n[00:07:00] And so you don’t have to do this manual pre-processing. Any sort of meta level reasoning, like natural language understanding, sentiment style. You want it a [00:07:10] positive document, you want a document in the style of a pirate, et cetera. I think they often include things that are multiple conditions you want in the style of a [00:07:20] pirate and you want a two page document or something like that.\nSo you’re gonna have these and or not conditions. and many more really. I think if it’s confusing to think about [00:07:30] instructions in Google search. I think the easiest thing to think about is that we’re so used to prompting language models. Let’s just treat our retrievers the exact same way. They use the [00:07:40] same underlying technology, we should be able to use all their capabilities. Okay, cool.\n\n\n\n[00:07:47] Orion Weller: So for this talk, I wanna talk about two different models [00:07:50] that kind of show this in two different aspects. One is an embedding model, it’s called prom reever. And I think this gets at this sort of fast, quick embeddable, [00:08:00] understand the whole meaning of what you’re saying. second is a more recent work called Rank One. And this uses a thinking reasoning language model that we train to be specific for search [00:08:10] and to be fast. Although it’s still much slower than of course an embedding model. So yeah, lemme talk about pro retriever. This was a work done [00:08:20] with Samaya AI and my collaborators at Johns Hopkins. And I wanted to start by briefly talking about these two categories of models. ’cause this is gonna come up a few [00:08:30] times. is this sort of buy encoder dense retriever, creates an embedding. So here you pass the text through the language model separately, like your query and document. [00:08:40] They’re both gonna create an embedding. at inference time, you’re gonna do some sort of quick dot product or cosign similarity to get your final score. So these are very scalable. [00:08:50] They’re very fast, but they’re not quite as expressive. On the other hand, we have these cross encoders. Sometimes we call them re rankers. It’s basically just a language model that you [00:09:00] give your query and document to at the same time, and that’s gonna output some score. Again very powerful, but it’s much slower compared to these buying coders. [00:09:10] And so for this work, what we wanted to do was, can we make these buying coders, these inventors take instructions? Can we make them profitable? Can we make them reason and [00:09:20] follow instructions? The key intuition that we did in this. The only thing that makes us different, there’s really only one thing that we added training [00:09:30] data for instruction following, and this is perhaps naive. You might think, oh, has no one really thought of that before. But it turns out that if you look at current [00:09:40] retrieval training data, it’s pretty much things like bing search logs.\nSo people are taking bing search logs in a dataset called Ms. Marco, then you fine tune on that. [00:09:50] Bing doesn’t take instructions, so no one is typing instructions into Bing. So we never learned this sort of capability. So what we did here is we created this [00:10:00] instruction training data so that you can take existing models that follow instructions like Llama and keep that instruction following ability. I’m not gonna talk really much about the [00:10:10] data. What we ended up doing is we had to synthetically generate it. So we took this query and positive document, passed it through a language model, and it generated this [00:10:20] instruction. And so we had to do this synthetically because there’s really not any existing data for it. But these days it’s not actually terribly hard to develop this sort of data. If you’re interested, there’s a [00:10:30] paper, the data’s also open source, et cetera. But yeah, let me show you how this does. So to make this really simple for the community, we started from a [00:10:40] model called Rep Lama, trains llama for embeddings. And so if you’re familiar with the mechanics of language models, what we actually did is, [00:10:50] Rep Lama does is they take the EOS token or the last token in the sequence and they fine tune the model to create a good sentence representation from that ES token. [00:11:00] We are going to use this same recipe that they did the same model everything for a direct comparison. We’re going to just add these instructions ’cause we wanna see how these [00:11:10] instructions help. We’re gonna evaluate on a lot of different types of data with instructions, data without instructions. And let me show you actually a few examples because I think that helps understand what the [00:11:20] problem is.\n\n\n\n[00:11:22] Orion Weller: The first one’s called Follower. And this is a data set that tests. If you’re like a user, you type into Google, you then change your mind, make a [00:11:30] change to Google you type it in. Again, it should update the search results to be closer to what you want in that new instruction. here you can see in this theory about Teflon, [00:11:40] they have this instruction they typed in. It used to be can be of any means, but they updated it and now it has to be related to chemicals. So if your retrieval model can follow [00:11:50] instructions, it should have more stuff related to these chemicals and less of being at a by means. And you can see this range from not following the instructions, doing the opposite negative a hundred [00:12:00] to a hundred, where it follows instructions perfectly to zero, where it just gives you the same results. And up to this point, no buying coder embedding model scored above zero. They [00:12:10] were pretty much all negative. They do the opposite of what you want. There’s also this other really cool data set called instructor. They have these different personas that they apply to the [00:12:20] query. I’m a student, my teacher wants me to look this up, et cetera.\nTo see if you can handle these different personas. So how does this model [00:12:30] do, I’m not gonna go too deep into results, but let me just share with you highlights here. So again, we’re comparing to rep Lama and. Rep Lama gets a negative score for instruction following pretty much [00:12:40] like every other, embedding, following embedding previously, prom tre for the first time gets this positive score, though we can see for the first time that embedding models really can follow [00:12:50] instructions here. On the instructor benchmark, we see again, it’s much better than the rep lama model that couldn’t that wasn’t trained with instructions. Let me show you the more interesting stuff. [00:13:00] What if you don’t have instructions? What if you are applying your new model? You have some new customers, you want to be able to use this sort of instruction, but you don’t really [00:13:10] know what to put there. So you could just use no prompt, like any existing retrieval model. That’s a great option. other option might be you come up with these like generic [00:13:20] prompts and maybe you even optimize.\nIf you have some small dev set, you’re gonna choose the best one there and apply it to your test set. Let me show you what we did when we came up with [00:13:30] these 10 prompts. Here are the actual prompts that we gave to the model. be careful when assigning relevance as your job is on the line. Think carefully about these conditions when determining relevance. [00:13:40] And so again, so we take any user query, we’re gonna add this little prompt at the end. and if the model understands instructions, it should be like, oh, this is like really important for the user. I need to [00:13:50] give them a good result. And performance will go up. If the model’s just trying to match a keyword performance will stay the same or get worse. And so it turns out if we do this on this [00:14:00] benchmark called beer, which Nandan talked about last time, you have no prompt, models perform about the same, which is a good sign, right? Like you wanna be able to perform well in [00:14:10] both settings. If you add this sort of generic prompt, we see, we get this nice gain. And rep lama that was trained without instructions. Doesn’t see any benefit. In fact, it gets [00:14:20] slightly worse, right? So you can literally just tell the model what you want. You can say you really need it to do well,\nAnd now performance can go up. this may sound weird, but this [00:14:30] is the easiest way to show that this aligns retrieval models with the language model community, right? Because you can prompt hack language models, you can tell them all sorts of things. Now you can do the same thing [00:14:40] to retrieval models. And that means they can also understand what you mean, if you took these prompts and you paraphrase them, they should be able to do equally well, right? [00:14:50] ’cause it’s still the same semantic meaning. And it turns out that if you take a keyword model like BM 25, of course you’re gonna have this really large range because it’s really sensitive [00:15:00] to keywords. Rep Lama is less sensitive to the keywords and prom retriever, which is trained to follow instructions, is much less sensitive. So these sorts of models understand the whole [00:15:10] meaning of what you’re saying rather than trying to pick out keywords or trying to match to something in like a paraphrase like way. with the right data, you can have retrievers that are prompted just [00:15:20] like a language model. having the sort of right data enables these sorts of capabilities because they’re already there in the language model.\nYou just have to keep them. And what’s most exciting to me about [00:15:30] this is it unlocks these new types of queries. You can ask for these meta level reasoning type things on top of documents that you just couldn’t ask before. Like you just couldn’t ask Google [00:15:40] about metaphors because it’s gonna try to keyword match it.\nBut now you have these sorts of systems that can do it. And you don’t need to be picky about the keywords. So you don’t need to [00:15:50] like try to help your users say the right things. You can just tell it what you want and the model will understand. And so that’s pro. It’s a fast [00:16:00] embedding model, and I think it’s the way that embeddings are gonna go in the future.\n\n\n\n[00:16:04] Orion Weller: The next one I wanted to talk about is rank one. And so this is the model that’s very strong, [00:16:10] but it’s slower because it’s, as I’ve talked about before across encoder here on the right. So it takes in the text together, which means that it’s gonna be slower [00:16:20] than these embedding models. So this is the type of thinking models. what the language model community really loves about these is that as you increase the amount of tokens, the amount [00:16:30] of words it does in its thinking performance goes up. So here’s oh one on a math data set, and as you increase the compute performance on this math data set goes from 20 to [00:16:40] 80. So they’ve really shown that these models provide a lot of huge gains to, on these hard tasks like code or science or other sorts of math questions. [00:16:50] And so we wanna bring those same gains to retrieval. I think if you’re not so familiar with these sort of thinking models, one way to think about this is it’s like a [00:17:00] long chain of thought and it’s pretty much used in every model these days.\nGemini, oh, one deep seek, they all use this sort of thing. Yeah. How would this [00:17:10] even look like in retrieval? So say you have a query and a document, do snow leopards change color? Your model is gonna output something like this. [00:17:20] it’s actually quite long, right? It’s long to read. Luckily, you don’t actually have to read it. It can be given to a different language model. You can just take the final output. Or if you’re really curious, you can read it. [00:17:30] so here, it thinks about whether. Leopards change colors. It even questions itself. You’re in blue, but wait, like maybe the user says this and they mean something different. And then [00:17:40] finally at the end it’s gonna tell you, oh, I actually don’t think this document’s relevant. It’s false. we want this to be fast and personalized for retrieval. we have the [00:17:50] procedure in the paper. Again, it’s actually quite simple.\nIt’s all about the data. So you get some data that can teach the model. To do this, you don’t even need to do any fancy reinforcement learning. It’s pretty [00:18:00] basic training. But let me show you the type of evaluation data and the type of things this can do. And so this is gonna mainly focus on the bride data set, What they did is [00:18:10] they have all these very unique relevance definitions. So instead of just having relevance be, does this answer my question, It has all these different ones, here at the bottom we have [00:18:20] math. So it has a math question and you actually wanna find a different problem that uses the same theorem to solve it. So it’s not trying to find the answer, it’s trying to find a different [00:18:30] question that uses the same theorem. Maybe you have code and you wanna find a different document that uses an alternate function. Or at this top one, you wanna find some [00:18:40] supporting evidence. So it uses all these very cool, unique ways of relevance to test if your model can do this reasoning and think through this definition of [00:18:50] relevance. So lemme, I threw an example into our model to show you. And so this was about could you find a similar lead code problem? I gave it the input and you can see [00:19:00] highlighted in red here that the model says, oh, this is a, this is the max area problem, and it uses a two pointer approach to solve it. this other problem that you’re giving me as a document [00:19:10] also uses a two pointer technique. So therefore, you know they are relevant to each other. They are similar. These models are really able to think about what you’re doing to be [00:19:20] very instructable to do this reasoning and give you your final output. It’s actually very impressive to me because I personally can’t just look at some of these things and be like, oh, like that’s a two pointer problem, and tell you off the top [00:19:30] of your head, but these models can, and so they’re able to unlock that. Yeah. Let me briefly show you some results here. We evaluate on a bunch of [00:19:40] things bright, the one I just showed you, a negation benchmark called never and a multilingual version of the follower benchmark I showed you before. This model I’m using as a baseline [00:19:50] was trained on 10 times more data, 6 million instead of 600,000. But yet adding this thinking goes from 14 to 27, so almost double [00:20:00] on bright, which is huge negation, understanding more than double. Then on instruction following, again, more than double. So we were blown away actually when we [00:20:10] saw this because it was just so much more effective. so I thought maybe like the data, like our data is just so much better. So we tested it with the same model, same data, but [00:20:20] with or without thinking. So without this reasoning chain during training. And it turns out that you see this 10 point gain simply from having the thinking. So training it to think is [00:20:30] like hugely effective. Let me just end by telling one fun story about this.\nSo there’s some old evaluation data that people use in retrieval. [00:20:40] It’s called DL 19 from the Trek Treks. And it was created in 2019. initially when we evaluated, we were surprised to see some really low [00:20:50] scores. when we dug into why that could be we realized that the documents that we’re finding had not been judged by humans. what that means is when IR researchers, [00:21:00] create these new evaluation sets. They take existing models, go out and find a bunch of documents, and then have humans go and annotate those documents, relevant or not, [00:21:10] until they run outta money there’s just too many documents for a human to go through all of them.\nSo you take a top set and you have a human annotate it. And it turned out that the [00:21:20] documents these previous models were finding had pretty much all been seen by those humans from the old systems Our model had a lot less of these judged documents. [00:21:30] So what we did to make this fair is we personally went through and every document that had never been judged by human, we went through and judged. What I love about this story actually is [00:21:40] that these thinking retrieval models are actually finding new documents that previous systems hadn’t found before. Which is really exciting to me because it means that it has this new, fresh [00:21:50] perspective.\nAnd it also probably indicates that the community should move on from these older data sets which I think everyone agrees with. There’s many more ones. This one that we were evaluating on was done before Bert, so [00:22:00] it’s very old. I think there’s growing consensus that we just shouldn’t use it anymore. But yeah, in summary, using this test time compute, this thinking makes these [00:22:10] profitable and reasoning re rankers, you don’t need any rl. It’s actually really simple to create these. are slower, but they’re much, much more powerful than previous approaches. [00:22:20] as just one example, we only trained on general web data. We didn’t train on instruction data. We didn’t train on multilingual data on any specific domains. So if you do [00:22:30] that, you’re gonna see some like huge gains. Yeah. And so that’s Frank one.\n\n\n\n[00:22:38] Orion Weller: Let me just conclude by headed back to what I started with.\n[00:22:40] So the goal is we want to have retrieval models that work just like language models, that we build our retrieval models these days on language models. They should have all of their [00:22:50] capabilities. They should understand what you mean. They should be able to reason, they should be able to follow your instructions. And so what does this actually mean for you if you’re a downstream user? If you have some [00:23:00] new application, you wanna have some new users on a new dataset, a new area? What it means is that these new retrievers benefit from advances in language models. So the next time you [00:23:10] see some really cool thing coming out from language model community it should be easily accessible to you for search, for rag, for whatever you’re doing.\nIt should be easily brought in. [00:23:20] And then the part I’m most excited about is all these new types of queries you can now give for rag and retrieval. You can just type anything you want in and it’ll be able to go find it, [00:23:30] give it back to you. You don’t need to try to keyword match. The language model will take care of it. And so I just wanna end by saying that all of these models are open data. They’re open [00:23:40] source. You can train them yourself, their MIT license. So feel free to use them for whatever. Thanks.\n\n\n\n[00:23:45] Hamel Husain: So just to make sure I understand, like when you were talking about prompt retriever, [00:23:50] I believe I recall you mentioned that you applied that to the buy encoder. It’s a, is that, did I recall correctly?\n\n\n\n[00:23:58] Orion Weller: Yeah, exactly.\n[00:23:58] Hamel Husain: Okay.\n[00:23:59] Orion Weller: Embeddings.\n[00:23:59] Hamel Husain: [00:24:00] And if I understand correctly, If you provide instructions on, how you wanna search, like you have that meta kind of instruction it changes the kind of [00:24:10] embedding that will be produced by the buy encoder. Is that the right understanding?\n[00:24:16] Orion Weller: Exactly. So you could give the query and the instruction as like one piece of [00:24:20] text and then it creates that single embedding.\n[00:24:22] Hamel Husain: Okay. so we have the buy coder. First question is, how do you. So like a lot of times with [00:24:30] Rag, you use a buy encoder and you like, you batch process a lot of your documents and you put them through, the buy encoder, you store the vectors of your [00:24:40] documents so that at inference time you can do that first pass with the buy encoder before you go to the cross encoder for the re-ranking.\nHow do you imagine like this would be [00:24:50] operationalized, let’s say. Like on inference time ’cause I imagine like your queries, this meta instruction, is it the, is what’s the idea? [00:25:00] Is it the idea that this meta instruction would be constant when you’re doing the kind of this like batch processing to store your documents in the vector [00:25:10] database?\nOr is there some other trick where, you know, you would. So like how do you actually operationalize this more fine-grained thing?\n[00:25:19] Orion Weller: the way [00:25:20] that we set it up is that the documents don’t get the instruction so you can. Batch process your documents just like you would any other retriever. And then at inference time, when you get a query [00:25:30] depending on your use case, you can either have the user type in a long query that can be its own instruction, or you can have some instruction in the backend that you append to that query. [00:25:40] And then it just creates that embedding. And you do the dot product with all the documents, Maybe the user has a specific instruction, so you wanna keep your documents instruction. They’re not influenced by the [00:25:50] instruction. They’re just separate in the date of soar. And then at inference time, the query gets the instruction and you do the DOT product.\n[00:25:56] Hamel Husain: okay, so the instruction is applied only to the query in a way. the [00:26:00] way it’s trained is like I. That’s really interesting. And thenyou mentioned the buy encoder. Why not also, make this part of the cross encoder just outta curiosity. [00:26:10] Maybe there’s a reason, but just wanted to ask.\n[00:26:12] Orion Weller: Yeah. great question. We actually do have some work doing that. I just didn’t choose to highlight it in this talk, but yeah, you totally can.\n[00:26:17] Hamel Husain: Sense.\n[00:26:18] Orion Weller: Yeah. I can also highlight, no, it’s [00:26:20] actually already even out with all the data. It’s part of the paper follow ir. It also has a benchmark, which we evaluated on this paper, but yeah it’s all out there as well. So yeah, feel free to check that if you’re interested [00:26:30] in a re ranker that can do that.\n[00:26:32] Hamel Husain: The next question I have is this meta search kind of instruction is extremely interesting, like you’re [00:26:40] giving. What are some nuances about the search that you should be if you’re, you are like searching for it as like an expert, what do you think is gonna happen?\nDo you think it’s gonna be [00:26:50] humans will provide that full thing or do you think LMS will be. Writing it for you, the meta or like what is your feeling, like that [00:27:00] might happen. What’s your like, ’cause you’ve been like working with this for some time.\n[00:27:05] Orion Weller: Yeah. I think it depends on the application, but I think both are really actually exciting and interesting [00:27:10] areas. Say you have a deep research system and you’re trying to go through it. I think having the deep research system know that it can add all these sorts of really precise [00:27:20] things will help it narrow in on exactly what it wants in sort of a rag application. on the other hand, if you’re just typing into Google and you’re not doing a deep research and you wanna be the power [00:27:30] user I think that’s helpful. But I actually do think there’s gonna be a ton of people where normal Google search works. But if you’re the sort of power user where you really wanna dial in and you wanna find exactly what you [00:27:40] want, and then if the model can take that sort of thing, I think it’ll really help them also.\n[00:27:45] Hamel Husain: That’s interesting. You mentioned deep research, like yeah, maybe you would have a UX where it [00:27:50] asks you follow up questions to help you build that meta prompt, to guide you. Okay, so I wanna ask you also about the other one, the more [00:28:00] expensive one. What was it called again?\n[00:28:02] Orion Weller: Yeah. Rank one.\n[00:28:03] Hamel Husain: Rank one. Okay. And so if I understand correctly, rank one basically is [00:28:10] like a reasoning model that you’ve trained to, basically. Be more powerful when it comes to dealing with [00:28:20] context and answering questions related to context. Is that correct\n[00:28:25] Orion Weller: Yeah. Finding document relevance.\n[00:28:27] Hamel Husain: how does it compare to reasoning models off the [00:28:30] shelf today? Like the frontier ones, let’s say like whatever 2.5 Pro oh, three Pro, whatever, if you’re going to if you put a prompt into those [00:28:40] reasoning models and say, Hey carefully consider if this document is relevant, blah, blah, blah, whatever.\nHave you, do you have any intuition on the baseline against that? How [00:28:50] is this, I think you have trained specifically for this purpose. What is, yeah, what’s the delta there?\n[00:28:55] Orion Weller: Also, yeah. Super interesting question. So we evaluated on some benchmarks [00:29:00] that O three has been evaluated on, and I think on those O three is like at 75 and our model is like at 69 ish. So there is a gap between the frontier [00:29:10] models there, but our model is again, like seven B. We have a 1.5 B version. And I think the models that we’re having are most useful if you want to like, control the sort of data distribution [00:29:20] you’re doing or you want it to be faster or something you control. If you are going for peak performance it’s gonna be hard to beat open AI there. But ours is much faster in that way and much smaller.[00:29:30]\n[00:29:30] Hamel Husain: That’s interesting. So perhaps, are you envisioning maybe like this smaller model would be more like in the rag pipeline itself and [00:29:40] you would then hand it off to O three? What is it like, I’m just guessing, but you tell me.\n[00:29:45] Orion Weller: I think it could be done that way too. I imagine if you’re dealing with a lot of queries, right? So I and [00:29:50] O three is really good at this sort of tool use. But I could see this sort of thing like Rank One model is being used as a tool that O three uses.\nHey, go do this. So you slowly build up, you [00:30:00] do your embedding, you do your re-ranking, then O three puts it all together. Or if it’s something like you just simply can’t use O three, like you have some government application, you have [00:30:10] some secret data, you can’t pass to O three these sort of things can be done much cheaper and there you can run it yourself so it’s secure in that way.\n[00:30:19] Hamel Husain: [00:30:20] Gotcha. What’s the easiest way for people? Okay. Actually before I get to that, is your intuition that we can use rank one kind of off the shelf for [00:30:30] across many domains. Do you think that people may want to, train it on their data sets? How easy is it to train on your data set? Can you talk about that a little bit?[00:30:40]\n[00:30:40] Orion Weller: Yeah, we were actually shocked at how well it worked without the training data for specific domains like it. Actually blew us away how good it generalized. However, I still think that means that if you give it the [00:30:50] data you want, it’s just gonna like really kill it. Was honestly so easy to train.\nIt’s just normal. Predict the next token loss. You just give it your data [00:31:00] predicts the next token, and it just. really good. So yeah, I, if you have something that you wanna use it for, I’d recommend just trying it off the shelf. It may just work amazing. If you wanna fine tune it [00:31:10] for your thing, it’s also easy and probably would work really well too.\n[00:31:13] Hamel Husain: Okay, so you’re saying like you supervise fine tuning basically, off the shelf for [00:31:20] against your data. Now, why is that the case? If it’s a reasoning model, wouldn’t you just outta curiosity just to make sure I understand how is it that you’re able to try and use the supervised fine tuning and why [00:31:30] not RL something or the other?\n[00:31:32] Orion Weller: that’s one thing that I actually really loved about that project. What did I have in the summary is that you don’t need that rl. So I think the Quinn [00:31:40] paper showed a bit of this in that if you have a smaller model, it can learn that distillation from the reasoning chase really easy. that’s, I think, the main reason.\nI’m speculating here, but it seems [00:31:50] to me to be the main reason why. Open AI doesn’t release the reasoning chains. Also, Gemini used to, I started actually gathering some from Gemini to train rank one, and then during [00:32:00] that process they turned it off. I think these companies have realized that it’s just so effective to use them training that they’ve completely turned them off.\n[00:32:09] Hamel Husain: [00:32:10] Okay. So where are you getting the reasoning tokens from now that they turned it off?\n[00:32:14] Orion Weller: Yeah. Nowadays you have to choose an open source alternative, right? So it’s\n[00:32:18] Hamel Husain: okay.\n[00:32:18] Orion Weller: gonna have to go to like deep [00:32:20] or something.\n[00:32:21] Hamel Husain: Okay. And if people wanna fiddle with this and try it for themselves on their applications, what’s like a minimal [00:32:30] setup that you recommend people do? Is this easy to wire up and Lance db and something? Is there a recipe that you recommend people look at?\n[00:32:39] Orion Weller: No [00:32:40] specific constraints on framework. I will say V-L-V-L-L-M is really effective for it. There’s actually a demo on hugging face right now for it. You can check it out. It has a GPU. You can test it out for [00:32:50] different queries you have, see how well it does. but otherwise it’s just like a normal language model, so any sort of framework that handles language models, VLM works great.\nAnd how can people find that [00:33:00] thing on hugging face? Rank one\n[00:33:01] Hamel Husain: Okay.\nname,\nOkay. Got it. Thanks for entertaining my questions. This is very exciting and very interesting [00:33:10] of a presentation, so thank you so much for doing this.\n[00:33:13] Orion Weller: Thank you so much for the opportunity, for the really great questions."
  },
  {
    "objectID": "orion_example/transcript.html#introduction-to-new-frontiers-in-information-retrieval",
    "href": "orion_example/transcript.html#introduction-to-new-frontiers-in-information-retrieval",
    "title": "Reasoning Opens up New Retrieval Frontiers",
    "section": "",
    "text": "[00:00:00]\n[00:00:00] Orion Weller: Today, I’ll be talking about new frontiers in retrieval or information retrieval ir, about instruction following and reasoning. If you are using language models today, like you’re [00:00:10] talking the chat, GPT there’s a couple things that have been huge for defining them and helping them be really useful for users."
  },
  {
    "objectID": "orion_example/transcript.html#the-power-of-instruction-following-in-language-models",
    "href": "orion_example/transcript.html#the-power-of-instruction-following-in-language-models",
    "title": "Reasoning Opens up New Retrieval Frontiers",
    "section": "",
    "text": "[00:00:18] Orion Weller: Two of the biggest ones I think [00:00:20] are instruction following. and if you’ve used a language model before, you can, it does all sorts of things. You can just tell it what you want and it will go out and do it. So here I ask [00:00:30] chat, GBT to generate a haiku about information retrieval in the style of a pirate and to mention rag. And it can follow my instructions perfectly.\nSo here’s this nice little haiku. [00:00:40] It mentions rag, it talks like a pirate. And this actually I feel like may be underwhelming to us today because we just expect them to do this, right? They’re [00:00:50] so good at instruction following, you can just tell a language model what you want, and it’ll give you what you want. But a few years ago, this was not the case."
  },
  {
    "objectID": "orion_example/transcript.html#advancements-in-reasoning-capabilities",
    "href": "orion_example/transcript.html#advancements-in-reasoning-capabilities",
    "title": "Reasoning Opens up New Retrieval Frontiers",
    "section": "",
    "text": "[00:00:58] Orion Weller: The other is reasoning, and this is something [00:01:00] that’s really hot in the community today. Pretty much any major player in language model space will have some reasoning model. And what sets these apart is that when you [00:01:10] ask it a question or type something in, it’s going to give these thinking tokens. So here’s one from Opening Eyes oh one, and here it thinks about how many Rs are in the word [00:01:20] strawberry.\nIt’ll generate these sort of intermediate tokens. And then finally it’ll end by telling you the final output after it thinks. so this is known as [00:01:30] reasoning, thinking, sometimes test time, compute all these sorts of things. And so reasoning and instruction following are two of the most major things that are making language [00:01:40] models so useful these days."
  },
  {
    "objectID": "orion_example/transcript.html#challenges-and-opportunities-in-modern-search-engines",
    "href": "orion_example/transcript.html#challenges-and-opportunities-in-modern-search-engines",
    "title": "Reasoning Opens up New Retrieval Frontiers",
    "section": "",
    "text": "[00:01:41] Orion Weller: What I wanna talk about today is how can we use these in retrieval? It’s like, how does this relate? have these really amazing language models. How can we bring them [00:01:50] over? If you look at Google in 1999, it looks like this, which is really actually quite similar to how [00:02:00] Google search looks today in 2025, So we have 26 years of development, it’s still very much in that you are gonna type words into a [00:02:10] box. It’s gonna go out and do some form of matching. Then it’s gonna come back and give you your list of websites. We do have nice new things like search GPT [00:02:20] these days. So if you see this, it’s a very nice ui. But what happens behind the scenes is it’s going to send your query to a search engine. In fact, often like Bing or Google [00:02:30] itself, just normal search, and then it’s gonna pass those results back to the language model, which then creates this very nice output. So [00:02:40] search itself really hasn’t changed very much. Search, GPT is still using a normal search engine. despite the fact that these days we’re using language [00:02:50] models, we’re training models even from Llama to be for retrieval, they still work pretty much the same. So we’re mostly just adding these [00:03:00] wrappers around the results of search. So what I wanna talk about today is pushing past this. Can we make retrieval use all the capabilities of the language model? How [00:03:10] can we unlock all these capabilities that we see that are going on in language models and bring them into retrieval? Lemme show some examples."
  },
  {
    "objectID": "orion_example/transcript.html#from-keyword-matching-to-semantic-search",
    "href": "orion_example/transcript.html#from-keyword-matching-to-semantic-search",
    "title": "Reasoning Opens up New Retrieval Frontiers",
    "section": "",
    "text": "[00:03:23] Orion Weller: back in [00:03:20] 1999, they mostly worked on search. This was before semantic search and mostly was just keyword matching, also known as lexical matching. [00:03:30] So you might have a query, like find websites explaining data privacy. You might also have some collection of documents. This could be things like data encryption [00:03:40] standards from nist, a government website, maybe a blog post called The Wolves Outside Your Data. And then another document called digital protection from a think [00:03:50] tank called Clear Law. And so if you were gonna pass these to a search engine from 1999, they used exact keyword search. You’re probably going to find the [00:04:00] first two documents, but not the third. And that’s because the third one here is like a synonym. It’s data and digital privacy and protection. So it wouldn’t match exactly. [00:04:10] However, we have come a long way in 25 years. So now we have semantic search, here what we do is we match it in semantic space. So [00:04:20] you’re going to go out and you’re gonna look for things like paraphrases and in the same similar space there. A good semantic search engine shouldn’t find all three of these. [00:04:30] So the next step I think is where we should be going with retrieval and where I feel like this new paradigm will really unlock a lot of great benefits."
  },
  {
    "objectID": "orion_example/transcript.html#instruction-based-search-a-new-paradigm",
    "href": "orion_example/transcript.html#instruction-based-search-a-new-paradigm",
    "title": "Reasoning Opens up New Retrieval Frontiers",
    "section": "",
    "text": "[00:04:38] Orion Weller: Lemme start off by talking about [00:04:40] instruction based search. imagine you have the same query, but you’re gonna add, you want a website that uses an extended metaphor to explain. [00:04:50] Here, there should only be one document that’s relevant, the one’s about wolves outside your data. But note here, you can’t do any form of keyword matching to find this document, [00:05:00] right? It’s not gonna say the word metaphor. It probably won’t even say a synonym of the word metaphor, like allegory or something like that. really have to understand the whole [00:05:10] document itself to be able to do this reasoning, to see that, oh, this is using a metaphor. So this is where I think instruction based search opens up these new categories of retrieval that you couldn’t [00:05:20] get before, any sort of meta document reasoning, But just to show how far you could push this I think you could move on. Oh, before I do that it’s important to note here that you can’t [00:05:30] solve this simply by using language models to reran. if you have a big collection, say a million documents. wanna find one with an extended metaphor. [00:05:40] If you just take the top a hundred or a thousand that could fit in a language model, you may not find them right because you’re not gonna be able to keyword match metaphor. [00:05:50] So I think this is really important that we still need these sort of embedding retrieval models to be able to find these because this is not something we could just entirely throw to a language model. There’s just so much [00:06:00] out there. So then the next one I think is out there, but I think really demonstrates how far you could push this, right?"
  },
  {
    "objectID": "orion_example/transcript.html#prompting-based-search-and-its-potential",
    "href": "orion_example/transcript.html#prompting-based-search-and-its-potential",
    "title": "Reasoning Opens up New Retrieval Frontiers",
    "section": "",
    "text": "[00:06:11] Orion Weller: And I’m gonna call this prompting based search or [00:06:10] reasoning based search. Here you could add on things like, I need you to have really high recall, or I’m going to lose my job. And so imagine you were just typing this into Google search, right? You’re gonna [00:06:20] type into Google search. I need you to have a really high recall. It’s definitely going to try to look for the word recall, It’s gonna try to keyword match that. again, we want [00:06:30] the model, which is a language model, to understand the whole intent of the query and be able to go out and find it.\nAnd this time it’ll think, oh, I need to have really good recall. I need to like, [00:06:40] be really careful here and hopefully it does better. Some areas I think it could help the neuro like document attributes. So these days we pre-process things. You add attributes like date, length, [00:06:50] source, all these sorts of things we have to manually add to documents. But an instruction based retriever should just be able to look at it and know, because it understands these things.\n[00:07:00] And so you don’t have to do this manual pre-processing. Any sort of meta level reasoning, like natural language understanding, sentiment style. You want it a [00:07:10] positive document, you want a document in the style of a pirate, et cetera. I think they often include things that are multiple conditions you want in the style of a [00:07:20] pirate and you want a two page document or something like that.\nSo you’re gonna have these and or not conditions. and many more really. I think if it’s confusing to think about [00:07:30] instructions in Google search. I think the easiest thing to think about is that we’re so used to prompting language models. Let’s just treat our retrievers the exact same way. They use the [00:07:40] same underlying technology, we should be able to use all their capabilities. Okay, cool."
  },
  {
    "objectID": "orion_example/transcript.html#introducing-prom-retriever-an-embedding-model",
    "href": "orion_example/transcript.html#introducing-prom-retriever-an-embedding-model",
    "title": "Reasoning Opens up New Retrieval Frontiers",
    "section": "",
    "text": "[00:07:47] Orion Weller: So for this talk, I wanna talk about two different models [00:07:50] that kind of show this in two different aspects. One is an embedding model, it’s called prom reever. And I think this gets at this sort of fast, quick embeddable, [00:08:00] understand the whole meaning of what you’re saying. second is a more recent work called Rank One. And this uses a thinking reasoning language model that we train to be specific for search [00:08:10] and to be fast. Although it’s still much slower than of course an embedding model. So yeah, lemme talk about pro retriever. This was a work done [00:08:20] with Samaya AI and my collaborators at Johns Hopkins. And I wanted to start by briefly talking about these two categories of models. ’cause this is gonna come up a few [00:08:30] times. is this sort of buy encoder dense retriever, creates an embedding. So here you pass the text through the language model separately, like your query and document. [00:08:40] They’re both gonna create an embedding. at inference time, you’re gonna do some sort of quick dot product or cosign similarity to get your final score. So these are very scalable. [00:08:50] They’re very fast, but they’re not quite as expressive. On the other hand, we have these cross encoders. Sometimes we call them re rankers. It’s basically just a language model that you [00:09:00] give your query and document to at the same time, and that’s gonna output some score. Again very powerful, but it’s much slower compared to these buying coders. [00:09:10] And so for this work, what we wanted to do was, can we make these buying coders, these inventors take instructions? Can we make them profitable? Can we make them reason and [00:09:20] follow instructions? The key intuition that we did in this. The only thing that makes us different, there’s really only one thing that we added training [00:09:30] data for instruction following, and this is perhaps naive. You might think, oh, has no one really thought of that before. But it turns out that if you look at current [00:09:40] retrieval training data, it’s pretty much things like bing search logs.\nSo people are taking bing search logs in a dataset called Ms. Marco, then you fine tune on that. [00:09:50] Bing doesn’t take instructions, so no one is typing instructions into Bing. So we never learned this sort of capability. So what we did here is we created this [00:10:00] instruction training data so that you can take existing models that follow instructions like Llama and keep that instruction following ability. I’m not gonna talk really much about the [00:10:10] data. What we ended up doing is we had to synthetically generate it. So we took this query and positive document, passed it through a language model, and it generated this [00:10:20] instruction. And so we had to do this synthetically because there’s really not any existing data for it. But these days it’s not actually terribly hard to develop this sort of data. If you’re interested, there’s a [00:10:30] paper, the data’s also open source, et cetera. But yeah, let me show you how this does. So to make this really simple for the community, we started from a [00:10:40] model called Rep Lama, trains llama for embeddings. And so if you’re familiar with the mechanics of language models, what we actually did is, [00:10:50] Rep Lama does is they take the EOS token or the last token in the sequence and they fine tune the model to create a good sentence representation from that ES token. [00:11:00] We are going to use this same recipe that they did the same model everything for a direct comparison. We’re going to just add these instructions ’cause we wanna see how these [00:11:10] instructions help. We’re gonna evaluate on a lot of different types of data with instructions, data without instructions. And let me show you actually a few examples because I think that helps understand what the [00:11:20] problem is."
  },
  {
    "objectID": "orion_example/transcript.html#evaluating-instruction-following-in-retrieval-models",
    "href": "orion_example/transcript.html#evaluating-instruction-following-in-retrieval-models",
    "title": "Reasoning Opens up New Retrieval Frontiers",
    "section": "",
    "text": "[00:11:22] Orion Weller: The first one’s called Follower. And this is a data set that tests. If you’re like a user, you type into Google, you then change your mind, make a [00:11:30] change to Google you type it in. Again, it should update the search results to be closer to what you want in that new instruction. here you can see in this theory about Teflon, [00:11:40] they have this instruction they typed in. It used to be can be of any means, but they updated it and now it has to be related to chemicals. So if your retrieval model can follow [00:11:50] instructions, it should have more stuff related to these chemicals and less of being at a by means. And you can see this range from not following the instructions, doing the opposite negative a hundred [00:12:00] to a hundred, where it follows instructions perfectly to zero, where it just gives you the same results. And up to this point, no buying coder embedding model scored above zero. They [00:12:10] were pretty much all negative. They do the opposite of what you want. There’s also this other really cool data set called instructor. They have these different personas that they apply to the [00:12:20] query. I’m a student, my teacher wants me to look this up, et cetera.\nTo see if you can handle these different personas. So how does this model [00:12:30] do, I’m not gonna go too deep into results, but let me just share with you highlights here. So again, we’re comparing to rep Lama and. Rep Lama gets a negative score for instruction following pretty much [00:12:40] like every other, embedding, following embedding previously, prom tre for the first time gets this positive score, though we can see for the first time that embedding models really can follow [00:12:50] instructions here. On the instructor benchmark, we see again, it’s much better than the rep lama model that couldn’t that wasn’t trained with instructions. Let me show you the more interesting stuff. [00:13:00] What if you don’t have instructions? What if you are applying your new model? You have some new customers, you want to be able to use this sort of instruction, but you don’t really [00:13:10] know what to put there. So you could just use no prompt, like any existing retrieval model. That’s a great option. other option might be you come up with these like generic [00:13:20] prompts and maybe you even optimize.\nIf you have some small dev set, you’re gonna choose the best one there and apply it to your test set. Let me show you what we did when we came up with [00:13:30] these 10 prompts. Here are the actual prompts that we gave to the model. be careful when assigning relevance as your job is on the line. Think carefully about these conditions when determining relevance. [00:13:40] And so again, so we take any user query, we’re gonna add this little prompt at the end. and if the model understands instructions, it should be like, oh, this is like really important for the user. I need to [00:13:50] give them a good result. And performance will go up. If the model’s just trying to match a keyword performance will stay the same or get worse. And so it turns out if we do this on this [00:14:00] benchmark called beer, which Nandan talked about last time, you have no prompt, models perform about the same, which is a good sign, right? Like you wanna be able to perform well in [00:14:10] both settings. If you add this sort of generic prompt, we see, we get this nice gain. And rep lama that was trained without instructions. Doesn’t see any benefit. In fact, it gets [00:14:20] slightly worse, right? So you can literally just tell the model what you want. You can say you really need it to do well,\nAnd now performance can go up. this may sound weird, but this [00:14:30] is the easiest way to show that this aligns retrieval models with the language model community, right? Because you can prompt hack language models, you can tell them all sorts of things. Now you can do the same thing [00:14:40] to retrieval models. And that means they can also understand what you mean, if you took these prompts and you paraphrase them, they should be able to do equally well, right? [00:14:50] ’cause it’s still the same semantic meaning. And it turns out that if you take a keyword model like BM 25, of course you’re gonna have this really large range because it’s really sensitive [00:15:00] to keywords. Rep Lama is less sensitive to the keywords and prom retriever, which is trained to follow instructions, is much less sensitive. So these sorts of models understand the whole [00:15:10] meaning of what you’re saying rather than trying to pick out keywords or trying to match to something in like a paraphrase like way. with the right data, you can have retrievers that are prompted just [00:15:20] like a language model. having the sort of right data enables these sorts of capabilities because they’re already there in the language model.\nYou just have to keep them. And what’s most exciting to me about [00:15:30] this is it unlocks these new types of queries. You can ask for these meta level reasoning type things on top of documents that you just couldn’t ask before. Like you just couldn’t ask Google [00:15:40] about metaphors because it’s gonna try to keyword match it.\nBut now you have these sorts of systems that can do it. And you don’t need to be picky about the keywords. So you don’t need to [00:15:50] like try to help your users say the right things. You can just tell it what you want and the model will understand. And so that’s pro. It’s a fast [00:16:00] embedding model, and I think it’s the way that embeddings are gonna go in the future."
  },
  {
    "objectID": "orion_example/transcript.html#rank-one-a-reasoning-based-retrieval-model",
    "href": "orion_example/transcript.html#rank-one-a-reasoning-based-retrieval-model",
    "title": "Reasoning Opens up New Retrieval Frontiers",
    "section": "",
    "text": "[00:16:04] Orion Weller: The next one I wanted to talk about is rank one. And so this is the model that’s very strong, [00:16:10] but it’s slower because it’s, as I’ve talked about before across encoder here on the right. So it takes in the text together, which means that it’s gonna be slower [00:16:20] than these embedding models. So this is the type of thinking models. what the language model community really loves about these is that as you increase the amount of tokens, the amount [00:16:30] of words it does in its thinking performance goes up. So here’s oh one on a math data set, and as you increase the compute performance on this math data set goes from 20 to [00:16:40] 80. So they’ve really shown that these models provide a lot of huge gains to, on these hard tasks like code or science or other sorts of math questions. [00:16:50] And so we wanna bring those same gains to retrieval. I think if you’re not so familiar with these sort of thinking models, one way to think about this is it’s like a [00:17:00] long chain of thought and it’s pretty much used in every model these days.\nGemini, oh, one deep seek, they all use this sort of thing. Yeah. How would this [00:17:10] even look like in retrieval? So say you have a query and a document, do snow leopards change color? Your model is gonna output something like this. [00:17:20] it’s actually quite long, right? It’s long to read. Luckily, you don’t actually have to read it. It can be given to a different language model. You can just take the final output. Or if you’re really curious, you can read it. [00:17:30] so here, it thinks about whether. Leopards change colors. It even questions itself. You’re in blue, but wait, like maybe the user says this and they mean something different. And then [00:17:40] finally at the end it’s gonna tell you, oh, I actually don’t think this document’s relevant. It’s false. we want this to be fast and personalized for retrieval. we have the [00:17:50] procedure in the paper. Again, it’s actually quite simple.\nIt’s all about the data. So you get some data that can teach the model. To do this, you don’t even need to do any fancy reinforcement learning. It’s pretty [00:18:00] basic training. But let me show you the type of evaluation data and the type of things this can do. And so this is gonna mainly focus on the bride data set, What they did is [00:18:10] they have all these very unique relevance definitions. So instead of just having relevance be, does this answer my question, It has all these different ones, here at the bottom we have [00:18:20] math. So it has a math question and you actually wanna find a different problem that uses the same theorem to solve it. So it’s not trying to find the answer, it’s trying to find a different [00:18:30] question that uses the same theorem. Maybe you have code and you wanna find a different document that uses an alternate function. Or at this top one, you wanna find some [00:18:40] supporting evidence. So it uses all these very cool, unique ways of relevance to test if your model can do this reasoning and think through this definition of [00:18:50] relevance. So lemme, I threw an example into our model to show you. And so this was about could you find a similar lead code problem? I gave it the input and you can see [00:19:00] highlighted in red here that the model says, oh, this is a, this is the max area problem, and it uses a two pointer approach to solve it. this other problem that you’re giving me as a document [00:19:10] also uses a two pointer technique. So therefore, you know they are relevant to each other. They are similar. These models are really able to think about what you’re doing to be [00:19:20] very instructable to do this reasoning and give you your final output. It’s actually very impressive to me because I personally can’t just look at some of these things and be like, oh, like that’s a two pointer problem, and tell you off the top [00:19:30] of your head, but these models can, and so they’re able to unlock that. Yeah. Let me briefly show you some results here. We evaluate on a bunch of [00:19:40] things bright, the one I just showed you, a negation benchmark called never and a multilingual version of the follower benchmark I showed you before. This model I’m using as a baseline [00:19:50] was trained on 10 times more data, 6 million instead of 600,000. But yet adding this thinking goes from 14 to 27, so almost double [00:20:00] on bright, which is huge negation, understanding more than double. Then on instruction following, again, more than double. So we were blown away actually when we [00:20:10] saw this because it was just so much more effective. so I thought maybe like the data, like our data is just so much better. So we tested it with the same model, same data, but [00:20:20] with or without thinking. So without this reasoning chain during training. And it turns out that you see this 10 point gain simply from having the thinking. So training it to think is [00:20:30] like hugely effective. Let me just end by telling one fun story about this.\nSo there’s some old evaluation data that people use in retrieval. [00:20:40] It’s called DL 19 from the Trek Treks. And it was created in 2019. initially when we evaluated, we were surprised to see some really low [00:20:50] scores. when we dug into why that could be we realized that the documents that we’re finding had not been judged by humans. what that means is when IR researchers, [00:21:00] create these new evaluation sets. They take existing models, go out and find a bunch of documents, and then have humans go and annotate those documents, relevant or not, [00:21:10] until they run outta money there’s just too many documents for a human to go through all of them.\nSo you take a top set and you have a human annotate it. And it turned out that the [00:21:20] documents these previous models were finding had pretty much all been seen by those humans from the old systems Our model had a lot less of these judged documents. [00:21:30] So what we did to make this fair is we personally went through and every document that had never been judged by human, we went through and judged. What I love about this story actually is [00:21:40] that these thinking retrieval models are actually finding new documents that previous systems hadn’t found before. Which is really exciting to me because it means that it has this new, fresh [00:21:50] perspective.\nAnd it also probably indicates that the community should move on from these older data sets which I think everyone agrees with. There’s many more ones. This one that we were evaluating on was done before Bert, so [00:22:00] it’s very old. I think there’s growing consensus that we just shouldn’t use it anymore. But yeah, in summary, using this test time compute, this thinking makes these [00:22:10] profitable and reasoning re rankers, you don’t need any rl. It’s actually really simple to create these. are slower, but they’re much, much more powerful than previous approaches. [00:22:20] as just one example, we only trained on general web data. We didn’t train on instruction data. We didn’t train on multilingual data on any specific domains. So if you do [00:22:30] that, you’re gonna see some like huge gains. Yeah. And so that’s Frank one."
  },
  {
    "objectID": "orion_example/transcript.html#conclusion-the-future-of-retrieval-models",
    "href": "orion_example/transcript.html#conclusion-the-future-of-retrieval-models",
    "title": "Reasoning Opens up New Retrieval Frontiers",
    "section": "",
    "text": "[00:22:38] Orion Weller: Let me just conclude by headed back to what I started with.\n[00:22:40] So the goal is we want to have retrieval models that work just like language models, that we build our retrieval models these days on language models. They should have all of their [00:22:50] capabilities. They should understand what you mean. They should be able to reason, they should be able to follow your instructions. And so what does this actually mean for you if you’re a downstream user? If you have some [00:23:00] new application, you wanna have some new users on a new dataset, a new area? What it means is that these new retrievers benefit from advances in language models. So the next time you [00:23:10] see some really cool thing coming out from language model community it should be easily accessible to you for search, for rag, for whatever you’re doing.\nIt should be easily brought in. [00:23:20] And then the part I’m most excited about is all these new types of queries you can now give for rag and retrieval. You can just type anything you want in and it’ll be able to go find it, [00:23:30] give it back to you. You don’t need to try to keyword match. The language model will take care of it. And so I just wanna end by saying that all of these models are open data. They’re open [00:23:40] source. You can train them yourself, their MIT license. So feel free to use them for whatever. Thanks."
  },
  {
    "objectID": "orion_example/transcript.html#qa-session",
    "href": "orion_example/transcript.html#qa-session",
    "title": "Reasoning Opens up New Retrieval Frontiers",
    "section": "",
    "text": "[00:23:45] Hamel Husain: So just to make sure I understand, like when you were talking about prompt retriever, [00:23:50] I believe I recall you mentioned that you applied that to the buy encoder. It’s a, is that, did I recall correctly?"
  },
  {
    "objectID": "orion_example/transcript.html#question-1-how-are-these-special-embeddings-operationalized",
    "href": "orion_example/transcript.html#question-1-how-are-these-special-embeddings-operationalized",
    "title": "Reasoning Opens up New Retrieval Frontiers",
    "section": "",
    "text": "[00:23:58] Orion Weller: Yeah, exactly.\n[00:23:58] Hamel Husain: Okay.\n[00:23:59] Orion Weller: Embeddings.\n[00:23:59] Hamel Husain: [00:24:00] And if I understand correctly, If you provide instructions on, how you wanna search, like you have that meta kind of instruction it changes the kind of [00:24:10] embedding that will be produced by the buy encoder. Is that the right understanding?\n[00:24:16] Orion Weller: Exactly. So you could give the query and the instruction as like one piece of [00:24:20] text and then it creates that single embedding.\n[00:24:22] Hamel Husain: Okay. so we have the buy coder. First question is, how do you. So like a lot of times with [00:24:30] Rag, you use a buy encoder and you like, you batch process a lot of your documents and you put them through, the buy encoder, you store the vectors of your [00:24:40] documents so that at inference time you can do that first pass with the buy encoder before you go to the cross encoder for the re-ranking.\nHow do you imagine like this would be [00:24:50] operationalized, let’s say. Like on inference time ’cause I imagine like your queries, this meta instruction, is it the, is what’s the idea? [00:25:00] Is it the idea that this meta instruction would be constant when you’re doing the kind of this like batch processing to store your documents in the vector [00:25:10] database?\nOr is there some other trick where, you know, you would. So like how do you actually operationalize this more fine-grained thing?\n[00:25:19] Orion Weller: the way [00:25:20] that we set it up is that the documents don’t get the instruction so you can. Batch process your documents just like you would any other retriever. And then at inference time, when you get a query [00:25:30] depending on your use case, you can either have the user type in a long query that can be its own instruction, or you can have some instruction in the backend that you append to that query. [00:25:40] And then it just creates that embedding. And you do the dot product with all the documents, Maybe the user has a specific instruction, so you wanna keep your documents instruction. They’re not influenced by the [00:25:50] instruction. They’re just separate in the date of soar. And then at inference time, the query gets the instruction and you do the DOT product.\n[00:25:56] Hamel Husain: okay, so the instruction is applied only to the query in a way. the [00:26:00] way it’s trained is like I. That’s really interesting. And thenyou mentioned the buy encoder. Why not also, make this part of the cross encoder just outta curiosity. [00:26:10] Maybe there’s a reason, but just wanted to ask.\n[00:26:12] Orion Weller: Yeah. great question. We actually do have some work doing that. I just didn’t choose to highlight it in this talk, but yeah, you totally can.\n[00:26:17] Hamel Husain: Sense.\n[00:26:18] Orion Weller: Yeah. I can also highlight, no, it’s [00:26:20] actually already even out with all the data. It’s part of the paper follow ir. It also has a benchmark, which we evaluated on this paper, but yeah it’s all out there as well. So yeah, feel free to check that if you’re interested [00:26:30] in a re ranker that can do that.\n[00:26:32] Hamel Husain: The next question I have is this meta search kind of instruction is extremely interesting, like you’re [00:26:40] giving. What are some nuances about the search that you should be if you’re, you are like searching for it as like an expert, what do you think is gonna happen?\nDo you think it’s gonna be [00:26:50] humans will provide that full thing or do you think LMS will be. Writing it for you, the meta or like what is your feeling, like that [00:27:00] might happen. What’s your like, ’cause you’ve been like working with this for some time.\n[00:27:05] Orion Weller: Yeah. I think it depends on the application, but I think both are really actually exciting and interesting [00:27:10] areas. Say you have a deep research system and you’re trying to go through it. I think having the deep research system know that it can add all these sorts of really precise [00:27:20] things will help it narrow in on exactly what it wants in sort of a rag application. on the other hand, if you’re just typing into Google and you’re not doing a deep research and you wanna be the power [00:27:30] user I think that’s helpful. But I actually do think there’s gonna be a ton of people where normal Google search works. But if you’re the sort of power user where you really wanna dial in and you wanna find exactly what you [00:27:40] want, and then if the model can take that sort of thing, I think it’ll really help them also.\n[00:27:45] Hamel Husain: That’s interesting. You mentioned deep research, like yeah, maybe you would have a UX where it [00:27:50] asks you follow up questions to help you build that meta prompt, to guide you. Okay, so I wanna ask you also about the other one, the more [00:28:00] expensive one. What was it called again?\n[00:28:02] Orion Weller: Yeah. Rank one.\n[00:28:03] Hamel Husain: Rank one. Okay. And so if I understand correctly, rank one basically is [00:28:10] like a reasoning model that you’ve trained to, basically. Be more powerful when it comes to dealing with [00:28:20] context and answering questions related to context. Is that correct\n[00:28:25] Orion Weller: Yeah. Finding document relevance.\n[00:28:27] Hamel Husain: how does it compare to reasoning models off the [00:28:30] shelf today? Like the frontier ones, let’s say like whatever 2.5 Pro oh, three Pro, whatever, if you’re going to if you put a prompt into those [00:28:40] reasoning models and say, Hey carefully consider if this document is relevant, blah, blah, blah, whatever.\nHave you, do you have any intuition on the baseline against that? How [00:28:50] is this, I think you have trained specifically for this purpose. What is, yeah, what’s the delta there?\n[00:28:55] Orion Weller: Also, yeah. Super interesting question. So we evaluated on some benchmarks [00:29:00] that O three has been evaluated on, and I think on those O three is like at 75 and our model is like at 69 ish. So there is a gap between the frontier [00:29:10] models there, but our model is again, like seven B. We have a 1.5 B version. And I think the models that we’re having are most useful if you want to like, control the sort of data distribution [00:29:20] you’re doing or you want it to be faster or something you control. If you are going for peak performance it’s gonna be hard to beat open AI there. But ours is much faster in that way and much smaller.[00:29:30]\n[00:29:30] Hamel Husain: That’s interesting. So perhaps, are you envisioning maybe like this smaller model would be more like in the rag pipeline itself and [00:29:40] you would then hand it off to O three? What is it like, I’m just guessing, but you tell me.\n[00:29:45] Orion Weller: I think it could be done that way too. I imagine if you’re dealing with a lot of queries, right? So I and [00:29:50] O three is really good at this sort of tool use. But I could see this sort of thing like Rank One model is being used as a tool that O three uses.\nHey, go do this. So you slowly build up, you [00:30:00] do your embedding, you do your re-ranking, then O three puts it all together. Or if it’s something like you just simply can’t use O three, like you have some government application, you have [00:30:10] some secret data, you can’t pass to O three these sort of things can be done much cheaper and there you can run it yourself so it’s secure in that way.\n[00:30:19] Hamel Husain: [00:30:20] Gotcha. What’s the easiest way for people? Okay. Actually before I get to that, is your intuition that we can use rank one kind of off the shelf for [00:30:30] across many domains. Do you think that people may want to, train it on their data sets? How easy is it to train on your data set? Can you talk about that a little bit?[00:30:40]\n[00:30:40] Orion Weller: Yeah, we were actually shocked at how well it worked without the training data for specific domains like it. Actually blew us away how good it generalized. However, I still think that means that if you give it the [00:30:50] data you want, it’s just gonna like really kill it. Was honestly so easy to train.\nIt’s just normal. Predict the next token loss. You just give it your data [00:31:00] predicts the next token, and it just. really good. So yeah, I, if you have something that you wanna use it for, I’d recommend just trying it off the shelf. It may just work amazing. If you wanna fine tune it [00:31:10] for your thing, it’s also easy and probably would work really well too.\n[00:31:13] Hamel Husain: Okay, so you’re saying like you supervise fine tuning basically, off the shelf for [00:31:20] against your data. Now, why is that the case? If it’s a reasoning model, wouldn’t you just outta curiosity just to make sure I understand how is it that you’re able to try and use the supervised fine tuning and why [00:31:30] not RL something or the other?\n[00:31:32] Orion Weller: that’s one thing that I actually really loved about that project. What did I have in the summary is that you don’t need that rl. So I think the Quinn [00:31:40] paper showed a bit of this in that if you have a smaller model, it can learn that distillation from the reasoning chase really easy. that’s, I think, the main reason.\nI’m speculating here, but it seems [00:31:50] to me to be the main reason why. Open AI doesn’t release the reasoning chains. Also, Gemini used to, I started actually gathering some from Gemini to train rank one, and then during [00:32:00] that process they turned it off. I think these companies have realized that it’s just so effective to use them training that they’ve completely turned them off.\n[00:32:09] Hamel Husain: [00:32:10] Okay. So where are you getting the reasoning tokens from now that they turned it off?\n[00:32:14] Orion Weller: Yeah. Nowadays you have to choose an open source alternative, right? So it’s\n[00:32:18] Hamel Husain: okay.\n[00:32:18] Orion Weller: gonna have to go to like deep [00:32:20] or something.\n[00:32:21] Hamel Husain: Okay. And if people wanna fiddle with this and try it for themselves on their applications, what’s like a minimal [00:32:30] setup that you recommend people do? Is this easy to wire up and Lance db and something? Is there a recipe that you recommend people look at?\n[00:32:39] Orion Weller: No [00:32:40] specific constraints on framework. I will say V-L-V-L-L-M is really effective for it. There’s actually a demo on hugging face right now for it. You can check it out. It has a GPU. You can test it out for [00:32:50] different queries you have, see how well it does. but otherwise it’s just like a normal language model, so any sort of framework that handles language models, VLM works great.\nAnd how can people find that [00:33:00] thing on hugging face? Rank one\n[00:33:01] Hamel Husain: Okay.\nname,\nOkay. Got it. Thanks for entertaining my questions. This is very exciting and very interesting [00:33:10] of a presentation, so thank you so much for doing this.\n[00:33:13] Orion Weller: Thank you so much for the opportunity, for the really great questions."
  },
  {
    "objectID": "orion_example/p3_orion.html",
    "href": "orion_example/p3_orion.html",
    "title": "P3: New Frontiers in IR - Instruction Following and Reasoning",
    "section": "",
    "text": "As part of our LLM Evals course, I hosted Orion Weller for the third part of our 5-part mini-series on evaluating and optimizing RAG. Orion is a researcher at Johns Hopkins University who focuses on the intersection of reasoning and information retrieval. His talk explores how we can move beyond traditional keyword and semantic search to create retrieval systems that leverage the advanced capabilities of modern LLMs, such as instruction following and reasoning. He demonstrates that by using the right training data and techniques, we can build retrievers that understand complex, natural language queries in the same way we prompt a language model.\nBelow is an annotated version of his presentation, with timestamped links for each slide.\n👉 We are teaching our last and final cohort of our AI Evals course next month (we have to get back to building). Here is a 35% discount code for readers of this post. 👈",
    "crumbs": [
      "orion_example",
      "P3: New Frontiers in IR - Instruction Following and Reasoning"
    ]
  },
  {
    "objectID": "orion_example/p3_orion.html#annotated-presentation",
    "href": "orion_example/p3_orion.html#annotated-presentation",
    "title": "P3: New Frontiers in IR - Instruction Following and Reasoning",
    "section": "Annotated Presentation",
    "text": "Annotated Presentation\n\n(Timestamp: 00:00:00)\nThe title slide for Orion’s talk, introducing the core themes of instruction following and reasoning in Information Retrieval (IR).\n\n(Timestamp: 00:00:07)\nOrion begins by highlighting the two capabilities that have been crucial for the success of modern language models like ChatGPT: instruction following and reasoning.\n\n(Timestamp: 00:00:20)\nThe first key capability is instruction following. Modern LLMs excel at understanding and executing complex, multi-part instructions given in natural language. Here, the prompt asks for a haiku about information retrieval, in the style of a pirate, that also mentions “RAG”.\n\n(Timestamp: 00:00:36)\nThe model successfully follows all instructions, generating a thematically appropriate haiku. Orion notes that while this capability seems standard today, it’s a recent advancement. A few years ago, models would not have been able to handle such a nuanced request.\n\n(Timestamp: 00:00:58)\nThe second capability is reasoning, also known as test-time compute or “thinking.” When given a query, these models don’t just produce a final answer; they generate a chain of thought or intermediate “thinking tokens” that show their process. This example from OpenAI’s o1 model shows it breaking down the problem of counting the letter ‘r’ in “strawberry” before giving the final answer.\n\n(Timestamp: 01:41)\nWith these powerful LLM capabilities established, Orion poses the central question of his talk: how can we integrate these advances into the field of information retrieval?\n\n(Timestamp: 01:52)\nOrion contrasts the Google search interface from 1999…\n\n(Timestamp: 01:58)\n…with the modern Google search interface. Despite 25+ years of development, he argues the fundamental user interaction has not changed much. You still type keywords into a box, and the system performs some form of matching to return a list of websites.\n\n(Timestamp: 02:17)\nEven with advanced interfaces like SearchGPT, the underlying process is often still based on traditional search. These systems typically send the user’s query to a standard search engine (like Bing or Google), retrieve the top results, and then pass those results to an LLM to generate a nice, conversational summary.\n\n(Timestamp: 02:38)\nThe core mechanism of search hasn’t fundamentally changed.\n\n(Timestamp: 02:46)\nOrion’s key point is that even when using powerful LLMs for retrieval (e.g., training a Llama model), we are mostly just adding a “wrapper” around the results of a traditional search process. The retrieval step itself does not yet fully leverage the instruction-following or reasoning capabilities of the underlying model. The goal is to push past this and build retrieval systems that can.\n\n(Timestamp: 03:16)\nTo illustrate the evolution, Orion starts with Keyword Search. This was the dominant paradigm in early search engines like those from 1999. It relies on exact or lexical matching of words in the query to words in the document.\n\n(Timestamp: 03:35)\nHe provides a sample query and a set of three documents.\n\n(Timestamp: 03:58)\nA keyword search would successfully match the first two documents, which contain the exact words “data” and “privacy.” However, it would fail to retrieve “Digital Protection” because it doesn’t understand that “digital” is related to “data” and “protection” is related to “privacy.”\n\n(Timestamp: 04:13)\nThe next step in the evolution is Semantic Search. This is the standard for modern systems, where models match text based on meaning (semantic space) rather than just keywords.\n\n(Timestamp: 04:26)\nA good semantic search engine can understand synonyms and paraphrases. Therefore, it correctly identifies that “Digital Protection” is relevant to the query about “data privacy” and retrieves all three documents.\n\n(Timestamp: 04:38)\nOrion introduces the next paradigm: Instruction-based Search. This goes beyond semantic matching to understand and execute specific instructions about the documents being retrieved. In this example, the user adds the instruction “and uses extended metaphors.”\n\n(Timestamp: 04:50)\nWith this instruction, the search system must now perform a meta-level analysis of the documents. It needs to not only find documents about data privacy but also reason about their writing style. Only the blog post “Wolves Outside Your Data” uses a metaphor, so it is the only relevant result.\n\n(Timestamp: 05:26)\nCrucially, this type of query cannot be solved by simply reranking the results of a standard semantic search. A semantic search for “data privacy” would retrieve the three documents, but none of them are likely to contain the keyword “metaphor.” The document that uses a metaphor would be missed in the initial retrieval stage, so a reranker would never even see it. This requires the retrieval model itself to understand the instruction.\n\n(Timestamp: 06:02)\nOrion pushes the concept further with Prompt and Reasoning-based Search. Here, the query includes not just a topic and instructions but also information about the user’s intent or constraints, such as “Have really high recall or I will lose my job.” A traditional search engine would mistakenly try to keyword-match “recall” or “job.” A true reasoning-based retriever would understand the user’s high-stakes need for comprehensive results and adjust its search strategy accordingly.\n\n(Timestamp: 06:42)\nSo, what constitutes an “instruction” in the context of Information Retrieval?\n\n(Timestamp: 06:45)\nInstructions can be about document attributes. Instead of pre-processing and filtering documents by date, length, or source, an instruction-based retriever should be able to understand these constraints directly from the user’s query (e.g., “find recent, short articles from government websites”).\n\n(Timestamp: 07:03)\nThey can also involve higher-level Natural Language Understanding (NLU) aspects, such as the sentiment or writing style of the content.\n\n(Timestamp: 07:15)\nInstructions can include complex logical conditions that combine multiple criteria (e.g., “find a positive document in the style of a pirate that is also two pages long”).\n\n(Timestamp: 07:26)\nThe scope of potential instructions is vast.\n\n(Timestamp: 07:31)\nOrion simplifies the concept: We are already used to prompting language models with complex, natural language requests.\n\n(Timestamp: 07:36)\nThe goal is to treat our retrievers the exact same way. Since they are built on the same underlying LLM technology, they should have the same capabilities.\n\n(Timestamp: 07:45)\nOrion introduces two models he will discuss that embody these new capabilities.\n\n(Timestamp: 07:48)\n\nPromptriever: A fast embedder that can be prompted to follow instructions during the initial, scalable retrieval stage.\n\n\n(Timestamp: 07:53)\n\nRank1: A strong but slow reranker that uses reasoning and test-time compute to make highly accurate relevance judgments.\n\n\n(Timestamp: 08:17)\nThis slide introduces the team behind Promptriever, a collaboration between Johns Hopkins and Samaya AI.\n\n(Timestamp: 08:23)\nOrion provides a quick overview of the two main architectures for retrieval models: - Bi-Encoder: This architecture creates separate embeddings for the query and the document. At inference time, a fast operation like cosine similarity is used to calculate the score. This is highly scalable but less expressive. - Cross-Encoder: This architecture processes the query and document together through a single LLM to produce a score. It is much more powerful and expressive but also much slower, making it suitable for reranking a smaller set of candidate documents.\n\n(Timestamp: 09:11)\nThe research question for Promptriever was: can we make the fast, scalable bi-encoder architecture take instructions?\n\n(Timestamp: 09:27)\nThe key insight was simple but powerful: the only thing needed was training data for instruction-following. Orion explains that existing retrieval training data (like MSMARCO, which is based on Bing search logs) doesn’t contain instructions because users don’t type instructions into traditional search engines. By creating this new type of data, they could teach an embedding model to follow instructions.\n\n(Timestamp: 10:07)\nThis slide shows an example of the training data generation process.\n\n(Timestamp: 10:10)\nStarting with an original query and a positive document pair from an existing dataset…\n\n(Timestamp: 10:16)\n…they used an LLM to generate a synthetic instruction that explains why the document is relevant to the query. This creates a new training triplet: (Instruction + Query, Positive Document). This process allows the model to learn the connection between a complex instruction and a relevant document.\n\n(Timestamp: 10:33)\nThe experimental settings for evaluating Promptriever.\n\n(Timestamp: 10:35)\nTo ensure a fair comparison, they started with RepLLaMA, a LLaMA-2 model fine-tuned for retrieval, and used its exact training recipe. The only change they made was adding their new instruction-based training data.\n\n(Timestamp: 10:43)\nThey evaluated the models on three types of data: the original in-domain data (MSMarco), new instruction-based data, and out-of-domain data to test generalization.\n\n(Timestamp: 11:20)\nThis section details the specific evaluation datasets used.\n\n(Timestamp: 11:23)\nThe first is FollowIR, a dataset designed to test if a model can follow updated instructions. A user issues a query, then refines it with an instruction. A good model should update its search results accordingly. The p-MRR metric measures this, with positive scores indicating successful instruction following and negative scores indicating the opposite.\n\n(Timestamp: 12:14)\nThe second dataset is InstructIR. It uses 10 different personas for each query (e.g., “I’m a student,” “I’m a professional”) to create a variety of natural, instruction-rich search scenarios.\n\n(Timestamp: 12:30)\nThis slide presents the results.\n\n(Timestamp: 12:33)\nThe chart shows the performance on instruction-following tasks.\n\n(Timestamp: 12:37)\nOn the FollowIR dataset, the baseline RepLLaMA scores negatively (-3.1), meaning it does the opposite of what the instruction asks. In contrast, Promptriever achieves a positive score (11.2), demonstrating for the first time that a bi-encoder embedding model can follow instructions.\n\n(Timestamp: 12:50)\nOn the InstructIR benchmark, Promptriever again significantly outperforms the baseline RepLLaMA, achieving a score of 63.1 compared to 50.2.\n\n(Timestamp: 12:59)\nBut what about performance on tasks without explicit instructions? This is a crucial test for ensuring the model hasn’t lost its general-purpose retrieval capabilities.\n\n(Timestamp: 13:11)\nTwo scenarios are tested: one with no prompt (the standard way of using a retriever) and one where a generic, task-agnostic prompt is added to the query.\n\n(Timestamp: 13:30)\nThese are the generic prompts used, such as “Be careful when assigning relevance as your job is on the line…” The idea is to see if the instruction-trained model can leverage these general hints to improve performance, while a standard model would likely be confused or degrade.\n\n(Timestamp: 13:58)\nThe results on the BEIR (out-of-domain) benchmark are telling. With no prompt, Promptriever performs slightly better than the baseline. However, when the generic prompt is added, Promptriever’s performance jumps significantly (from 55.0 to 56.4), while the baseline model’s performance slightly degrades. This shows that the instruction-trained model can understand and benefit from prompts, even generic ones.\n\n(Timestamp: 14:45)\nThis box plot shows the standard deviation of performance across 10 different generic prompts. A lower standard deviation indicates that the model is less sensitive to the specific wording of the prompt and better understands the underlying semantic intent.\n\n(Timestamp: 14:52)\nPromptriever shows a much smaller variance compared to both the keyword-based BM25 and the standard semantic model, RepLLaMA. This demonstrates that it is more robust and truly understands the meaning of the instructions, rather than just matching keywords within the prompt.\n\n(Timestamp: 15:16)\nOrion summarizes the key takeaways for Promptriever: - With the right data, even fast bi-encoder retrievers can be made promptable like LLMs. - This unlocks new types of queries that go beyond simple semantic relevance. - You no longer need to be picky about keywords; you can just tell the model what you want in natural language.\n\n(Timestamp: 15:57)\nNow, the presentation shifts focus from the fast embedder (Promptriever) to the strong but slow reranker (Rank1).\n\n(Timestamp: 16:05)\nThis slide introduces Rank1, a model designed to bring test-time compute (reasoning) to information retrieval.\n\n(Timestamp: 16:13)\nRank1 is a cross-encoder model. As a reminder, this architecture processes the query and document together, allowing for deeper interaction and more nuanced relevance judgments, but at a higher computational cost.\n\n(Timestamp: 16:22)\nThe goal is to leverage the power of “thinking” models for retrieval. The plot on the right, from OpenAI’s o1 model, shows that as you increase the amount of test-time compute (the length of the reasoning chain), the model’s accuracy on hard tasks like math problems increases dramatically. Rank1 aims to bring this same benefit to IR.\n\n(Timestamp: 17:08)\nWhat does this test-time compute look like in an IR context?\n\n(Timestamp: 17:12)\nGiven a query (“do snow leopards change color”) and a document, the model generates a detailed reasoning chain.\n\n(Timestamp: 17:18)\nThe model breaks down its thought process. It identifies the key term “varies” and considers its possible interpretations. It even questions its own initial assumption (“But wait, ‘varies’ might just mean…”), a hallmark of sophisticated reasoning. Finally, it concludes that the document is not directly relevant and outputs “false.” This entire reasoning chain is generated by the model to arrive at its final score.\n\n(Timestamp: 18:01)\nThe presentation moves on to the evaluation data for Rank1.\n\n(Timestamp: 18:06)\nThe main evaluation is done on the BRIGHT dataset, which contains unique and challenging relevance definitions. For example, a math query might require finding a document that uses the same theorem to solve a different problem. A code query might ask for a document with an alternative function. These tasks require deep reasoning that goes far beyond keyword or simple semantic matching.\n\n(Timestamp: 18:50)\nThis slide shows the model’s reasoning process for a LeetCode problem. The task is to find a similar problem. The model identifies that both problems involve a “two-pointer approach,” a specific algorithmic technique. It correctly reasons that because they share this underlying algorithm, they are similar, and thus the document is relevant. Orion notes that this is an impressive level of reasoning that even a human might struggle with.\n\n(Timestamp: 19:35)\nThis slide presents the results of the evaluation.\n\n(Timestamp: 19:38)\nRank1 is evaluated on a broad range of tasks that test reasoning (BRIGHT), negation understanding (NevIR), and instruction following (mFollowIR).\n\n(Timestamp: 19:48)\nAn important note: the baseline model, RankLLaMA, was trained on 10 times more data than Rank1.\n\n(Timestamp: 19:55)\nDespite being trained on less data, Rank1 significantly outperforms the baseline across all tasks. On the BRIGHT reasoning benchmark, it nearly doubles the score. On negation and instruction following, it more than doubles the score. This demonstrates the immense power of training a model to explicitly reason.\n\n(Timestamp: 20:16)\nTo isolate the impact of the reasoning chain itself, they ran an experiment with the exact same model and data, but toggled the reasoning chain on and off during training.\n\n(Timestamp: 20:24)\nThe results are stark. Simply training the model to generate the reasoning chain before the final answer results in a 10-point gain in performance. Explicitly teaching the model to “think” is hugely effective.\n\n(Timestamp: 20:33)\nOrion shares an interesting story about evaluating on older datasets.\n\n(Timestamp: 20:44)\nWhen initially testing on the DL19/DL20 datasets (from 2019), they were surprised to see very low scores. Upon investigation, they found that the number of “judged” documents their model retrieved was significantly lower than for previous models.\n\n(Timestamp: 20:52)\nThis bar chart shows the initial low scores for their model (Rank1-7B) compared to others.\n\n(Timestamp: 21:31)\nTo correct for this, the researchers manually re-judged all the unjudged documents that their models retrieved.\n\n(Timestamp: 21:38)\nThe key finding: Rank1 was finding new, relevant documents that older systems had completely missed. Because these documents weren’t in the original judgment pool, they were incorrectly marked as irrelevant, deflating the score. After re-judging, Rank1’s performance is shown to be state-of-the-art. This demonstrates that reasoning-based models have a fresh perspective and can uncover information that previous paradigms could not.\n\n(Timestamp: 21:50)\nThis also serves as a cautionary tale. The community should probably move on from older evaluation datasets like DL19, which was created before the BERT era and is biased towards the types of documents that older systems could find.\n\n(Timestamp: 22:05)\nOrion summarizes the takeaways for Rank1: - Using test-time compute (thinking) creates promptable and reasoning rerankers without needing complex reinforcement learning. - While slower, these models are much more powerful than previous approaches. - The models were only trained on general web data; training on specific in-domain data would likely lead to even larger gains.\n\n(Timestamp: 22:36)\nThe overall goal is to create IR systems that work just like LLMs. By combining fast, promptable embedders (like Promptriever) with strong, reasoning-based rerankers (like Rank1), we can build systems that understand complex, multi-faceted queries and retrieve highly relevant, nuanced results.\n\n(Timestamp: 22:56)\nWhat does this new paradigm mean for practitioners?\n\n(Timestamp: 23:05)\nFirst, it means that new retrievers will directly benefit from advances in LLMs. As base models get better at reasoning and instruction following, our retrieval systems will inherit those capabilities.\n\n(Timestamp: 23:19)\nSecond, it means we can move towards true instruction-based search. Users can type any query they can imagine, with all its nuance and complexity, and the system will be able to understand and search for it, eliminating the need for careful keyword engineering.\n\n(Timestamp: 23:36)\nOrion concludes by noting that all the models and data discussed are open-source and available for use under an MIT license.",
    "crumbs": [
      "orion_example",
      "P3: New Frontiers in IR - Instruction Following and Reasoning"
    ]
  },
  {
    "objectID": "orion_example/p3_orion.html#qa-session",
    "href": "orion_example/p3_orion.html#qa-session",
    "title": "P3: New Frontiers in IR - Instruction Following and Reasoning",
    "section": "Q&A Session",
    "text": "Q&A Session\n\nHow does Promptriever work with pre-processed documents? Does the instruction change the document embeddings? The instruction is only applied to the query, not the documents. You can batch-process and embed your entire document corpus once, just as you would with a standard retriever. At inference time, the instruction is appended to the user’s query, and a single, new embedding is generated for that combined text. This new query embedding is then used to search against the static document embeddings. This design ensures scalability while still allowing for dynamic, instruction-based queries.\nCan these instruction-following techniques be applied to cross-encoders as well? Yes, absolutely. Orion mentioned they have other work (FollowIR) that does exactly this, creating a reranker that can follow instructions.\nWho provides the meta-instructions for search? Will it be humans or other LLMs? Orion believes it will be both. For complex, agentic RAG applications (like a “deep research system”), an LLM might generate a precise, detailed instruction to guide the retrieval step. For end-user applications, a “power user” could type in a complex natural language query with their own instructions to get exactly what they want. In some cases, a UI could even have follow-up questions to help the user build a more effective meta-prompt.\nHow does Rank1 compare to frontier reasoning models like OpenAI’s o3 or Gemini? There is still a performance gap. On benchmarks where they can be compared, o3 scores around 75 while the 7B parameter Rank1 model scores around 69. However, Rank1 is much smaller and fully open-source, making it suitable for applications where you need to control the data, cost, speed, or run the model yourself. The gap exists, but the open-source model is still highly capable.\nHow can people get started with these models? Both Promptriever and Rank1 are open-source with MIT licenses. For Rank1, there is an interactive demo on Hugging Face where you can test it with your own queries. It can be used off-the-shelf with standard frameworks like vLLM.\nWhy is Rank1 easy to train with supervised fine-tuning instead of more complex methods like Reinforcement Learning (RL)? This is a key finding. The model learns the reasoning capability very effectively through simple next-token prediction on training data that includes the reasoning chains. There is no need for complex RL. Orion speculates that this distillation of reasoning is so effective that it’s likely why companies like OpenAI and Google have stopped exposing their models’ full reasoning chains, as it makes it too easy for others to train smaller, capable models.\n\n\n👉 We are teaching our last and final cohort of our AI Evals course next month (we have to get back to building). Here is a 35% discount code for readers of this post. 👈",
    "crumbs": [
      "orion_example",
      "P3: New Frontiers in IR - Instruction Following and Reasoning"
    ]
  },
  {
    "objectID": "orion_example/p3_orion.html#video",
    "href": "orion_example/p3_orion.html#video",
    "title": "P3: New Frontiers in IR - Instruction Following and Reasoning",
    "section": "Video",
    "text": "Video\nHere is the full video:",
    "crumbs": [
      "orion_example",
      "P3: New Frontiers in IR - Instruction Following and Reasoning"
    ]
  },
  {
    "objectID": "gem.html",
    "href": "gem.html",
    "title": "gem",
    "section": "",
    "text": "This notebook provides a minimal interface to Google’s Gemini API. The goal is to make it dead simple to:\nAll through a single gem() function that just works.",
    "crumbs": [
      "gem"
    ]
  },
  {
    "objectID": "gem.html#setup",
    "href": "gem.html#setup",
    "title": "gem",
    "section": "Setup",
    "text": "Setup\nFirst, make sure you have your Gemini API key set:\n\n# export GEMINI_API_KEY='your-api-key'\nassert os.environ.get(\"GEMINI_API_KEY\"), \"Please set GEMINI_API_KEY environment variable\"",
    "crumbs": [
      "gem"
    ]
  },
  {
    "objectID": "gem.html#building-blocks",
    "href": "gem.html#building-blocks",
    "title": "gem",
    "section": "Building blocks",
    "text": "Building blocks\nLet’s start with the simple helper functions that make everything work.\n\nClient creation\nWe need a Gemini client to talk to the API:\n\n\nConverting attachments to Parts\nGemini expects different types of content (files, URLs) to be wrapped in “Parts”. This helper handles that conversion:",
    "crumbs": [
      "gem"
    ]
  },
  {
    "objectID": "gem.html#the-main-interface",
    "href": "gem.html#the-main-interface",
    "title": "gem",
    "section": "The main interface",
    "text": "The main interface\nNow we can build our main gem() function that handles all use cases:\n\nsource\n\ngem\n\n gem (prompt, o=None, model='gemini-2.5-flash', thinking=-1, search=False)\n\nGenerate content with Gemini\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprompt\n\n\nText prompt\n\n\no\nNoneType\nNone\nOptional file/URL attachment or list of attachments\n\n\nmodel\nstr\ngemini-2.5-flash\n\n\n\nthinking\nint\n-1\n\n\n\nsearch\nbool\nFalse",
    "crumbs": [
      "gem"
    ]
  },
  {
    "objectID": "gem.html#examples",
    "href": "gem.html#examples",
    "title": "gem",
    "section": "Examples",
    "text": "Examples\nOne function handles everything: - Just text? Pass a prompt. - Have a file? Pass it as the second argument. - Got a YouTube URL? Same thing.\nLet’s test it out:\n\nText generation\nThe simplest case - just generate some text:\n\ngem(\"Write a haiku about Python programming\")\n\n'Clean, clear code unfolds,\\nIndents guide the powerful flow,\\nProblems solved with ease.'\n\n\n\n\nVideo analysis\nPerfect for creating YouTube chapters or summaries:\n\nprompt = \"5 word summary of this video.\"\ngem(prompt, \"https://youtu.be/1x3k0V2IITo\")\n\n'Late interaction beats single vector.'\n\n\n\n\nFile analysis\nGreat for extracting information from PDFs or images:\n\ngem(\"3 sentence summary of this presentation.\", \"NewFrontiersInIR.pdf\")\n\n'This presentation introduces new frontiers in Information Retrieval (IR), focusing on instruction following and reasoning capabilities, much like Large Language Models (LLMs). It presents two key models: Promptriever, a fast bi-encoder trained to follow natural language instructions for retrieval, and Rank1, a slower cross-encoder capable of complex, test-time reasoning for judging document relevance. These \"promptable\" and \"reasoning\" retrievers significantly enhance search performance, unlock new types of queries, and can even uncover previously overlooked relevant documents.'\n\n\n\ngem(\"What's in this image?\", \"anton.png\")\n\n'This image is a digital graphic, likely a thumbnail for a video or presentation, set against a dark blue or black background.\\n\\nHere\\'s a detailed breakdown of its contents:\\n\\n1.  **Person:** On the lower-left side, a young Caucasian man with short brown hair is smiling broadly, showing his teeth. He is wearing a light-colored (possibly white) V-neck shirt, with his head and upper chest visible.\\n2.  **Emoji:** Above and slightly to the left of the man\\'s head, there is a yellow \"sad face\" emoji with downturned eyes and mouth.\\n3.  **Text:**\\n    *   In the upper-left, white text reads: \"Single Vector?\"\\n    *   Below this, and slightly to the right of the emoji, large, bold yellow text is stacked vertically:\\n        *   \"YOU\\'RE\"\\n        *   \"MISSING\"\\n        *   \"OUT\"\\n    *   Combined, the main text reads: \"Single Vector? YOU\\'RE MISSING OUT\"\\n4.  **Diagram/Network:** On the right side, there\\'s an abstract glowing blue diagram resembling a neural network or a data flow graph. It features multiple interconnected blue circular nodes (or \"neurons\") linked by glowing blue lines.\\n    *   There appears to be an upper cluster of nodes and a lower cluster.\\n    *   An arrow points upwards from the lower cluster towards the upper cluster.\\n    *   Another arrow points downwards from the upper cluster towards a dark blue rectangular box with rounded corners.\\n5.  **\"RAG\" Box:** Inside the blue rectangular box on the lower-right, the white, bold capital letters \"RAG\" are prominently displayed.\\n\\nThe overall impression is that of a promotional image, possibly related to technology, artificial intelligence, or data processing, with the text suggesting a warning or a missed opportunity for those who stick to \"single vector\" approaches, implying that the RAG (Retrieval-Augmented Generation) concept shown in the diagram is a superior method.'\n\n\n\n\nChange Model\nYou can also control the model and thinking time:\n\ngem(\"What is Hamel Husain's current job?\", model=\"gemini-2.5-pro\")\n\n\"Based on his public profiles and professional presence, Hamel Husain's current job is **Co-founder and CEO** of **Gantry**.\\n\\nGantry is a company that provides an AI observability platform designed to help teams monitor, analyze, and improve their machine learning models in production.\\n\\nBefore co-founding Gantry in 2022, he was well-known for his role as the Head of Machine Learning at GitHub.\"\n\n\n\n\nGrounded Search\nAs you can see, grounded search is required to get things right sometimes!\n\ngem(\"What is Hamel Husain's current job?.\", search=True)\n\n\"\\nthought\\nThe search results indicate that Hamel Husain is currently an independent consultant specializing in AI and machine learning, particularly in operationalizing Large Language Models (LLMs). While he was previously a Staff Machine Learning Engineer at GitHub and is still listed as such in some contexts, more recent information from March and July 2025 explicitly states his role as an independent consultant. This suggests his independent consulting is his current primary job. Therefore, I have sufficient information to answer the user's request.Hamel Husain is currently an independent consultant, specializing in helping companies build, evaluate, and operationalize AI-powered systems and Large Language Models (LLMs). He focuses on making AI more reliable, understandable, and actionable.\\n\\nPreviously, he held the position of Staff Machine Learning Engineer at GitHub, where he was involved in the design and development of software engineering, machine learning, and developer tools, and led engineering teams. His work at GitHub included leading research efforts on projects like CodeSearchNet, which was a precursor to GitHub Copilot. He has also worked at companies such as Airbnb, DataRobot, and Accenture.Hamel Husain is currently an independent consultant, running Parlance Labs, where he helps companies build AI products and operationalize Large Language Models (LLMs). He also serves on the R&D team at AnswerAI and scouts for Bain Capital.\\n\\nPreviously, he was a Staff Machine Learning Engineer at GitHub, where he led research efforts on projects such as CodeSearchNet, a precursor to GitHub Copilot. His extensive experience in machine learning spans over 20 years, with past roles at companies like Airbnb, DataRobot, and Accenture.\"\n\n\n\n\nMultiple Attachments\nYou can analyze multiple files/URLs at once by passing a list:\n\nprompt = \"Is this PDF and YouTube video related or are they different talks? Answer with very short yes/no answer.\"\ngem(prompt, [\"https://youtu.be/Trps2swgeOg?si=yK7CO0Zk4E1rfp6s\", \"NewFrontiersInIR.pdf\"])\n\n'No.'\n\n\n\ngem(prompt, [\"https://youtu.be/YB3b-wPbSH8?si=WI0LqflY5SYIsRz9\", \"NewFrontiersInIR.pdf\"])\n\n'Yes.'",
    "crumbs": [
      "gem"
    ]
  },
  {
    "objectID": "gem.html#shortcuts",
    "href": "gem.html#shortcuts",
    "title": "gem",
    "section": "Shortcuts",
    "text": "Shortcuts\n\nFunctions to help you do common tasks\n\nsource\n\n\nyt_chapters\n\n yt_chapters (link)\n\nGenerate YoutTube Summary and Chapters From A Public Video.\nThis is what it looks like for Antoine’s Late Interaction Talk:\n\nchp = yt_chapters(\"https://youtu.be/1x3k0V2IITo\")\nprint(chp)\n\nAntoine Chaffin explains the inherent limitations of single vector search, such as information loss from pooling and poor performance in out-of-domain and long-context scenarios. He then introduces late interaction (multi-vector) models as a superior alternative that avoids these pitfalls and presents the PyLate library to make training and evaluating these powerful models more accessible.\n\n00:00 - Going Further: Late Interaction Beats Single Vector Limits\n00:32 - About Antoine Chaffin\n01:40 - Explaining Dense (Single) Vector Search\n03:08 - Why Single Vector Search is the Go-To for RAG\n03:54 - Performance Evaluation and the MTEB Leaderboard\n04:17 - BEIR: A School Case of Goodhart's Law\n05:36 - Limitations Beyond Standard Benchmarks\n08:24 - Pooling: The Intrinsic Flaw of Dense Models\n08:41 - How Pooling Creates Problems in Production\n10:42 - The Advantage of BM25\n11:32 - Replacing Pooling with Late Interaction\n12:17 - Why Not Just Use a Bigger Single Vector?\n13:51 - Performance Comparison: Late Interaction vs. Dense Models\n16:48 - Interpretability: A Nice Little Bonus\n17:42 - Why Are People Still Using Dense Models?\n18:43 - PyLate: Extending Sentence Transformers for Multi-Vector Models\n21:28 - Evaluating Models with PyLate\n22:49 - Future Avenues for Research\n24:36 - Conclusion and Resources\n25:51 - Q&A: Latency of Late Interaction vs. Dense Vector\n31:00 - Q&A: Fine-tuning Comparison\n33:20 - Q&A: Tips for Fine-tuning with PyLate",
    "crumbs": [
      "gem"
    ]
  },
  {
    "objectID": "yt.html",
    "href": "yt.html",
    "title": "yt",
    "section": "",
    "text": "Automate pesky chapter creation + description\n\nsource\n\n\n\n yt_chapters (link)\n\nGenerate YoutTube Summary and Chapters From A Public Video.\nThis is what it looks like for Antoine’s Late Interaction Talk:\n\nchp = yt_chapters(\"https://youtu.be/1x3k0V2IITo\")\nprint(chp)\n\nIn this presentation, Antoine Chaffin explains the inherent limitations of single-vector search, such as information loss from pooling and poor out-of-domain performance, and introduces late interaction (multi-vector) models as a superior solution. He demonstrates how these models excel in long-context and reasoning-intensive tasks and presents the PyLate library to make training and evaluating these powerful models more accessible.\n\n00:00 - Introduction\n00:32 - About Me\n01:40 - Dense (Single) Vector Search Explained\n03:08 - Single Vector Models: The Go-To for RAG\n03:55 - Performance Evaluation & MTEB Leaderboard\n04:17 - The BEIR Benchmark & Goodhart's Law\n05:36 - Limitations Beyond Benchmarks: The Long Context Problem\n06:33 - Limitations Beyond Benchmarks: Reasoning-Intensive Retrieval\n07:50 - The Role of BM25\n08:24 - Pooling: The Intrinsic Flaw of Dense Models\n11:32 - Replacing Pooling with Late Interaction\n12:17 - Why Not Just Use a Bigger Single Vector?\n13:51 - Late Interaction: A Simple, Yet Effective, Difference\n16:48 - Interpretability: A Nice Little Bonus\n17:42 - Why Are People Still Using Dense Models?\n18:43 - PyLate: Extending Sentence Transformers for Multi-Vector Models\n21:28 - Training is Cool, Show Me the Evals\n22:49 - What Are the Future Avenues?\n24:36 - Conclusion & QR Codes\n25:52 - Q&A: Latency of Late Interaction vs. Dense Vector Models\n31:00 - Q&A: Does Fine-Tuning Close the Performance Gap?\n33:20 - Q&A: How Easy is it to Fine-Tune with PyLate?\n34:22 - Q&A: Common Mistakes When Moving from Single to Multi-Vector?",
    "crumbs": [
      "yt"
    ]
  },
  {
    "objectID": "yt.html#youtube-chapter-creation",
    "href": "yt.html#youtube-chapter-creation",
    "title": "yt",
    "section": "",
    "text": "Automate pesky chapter creation + description\n\nsource\n\n\n\n yt_chapters (link)\n\nGenerate YoutTube Summary and Chapters From A Public Video.\nThis is what it looks like for Antoine’s Late Interaction Talk:\n\nchp = yt_chapters(\"https://youtu.be/1x3k0V2IITo\")\nprint(chp)\n\nIn this presentation, Antoine Chaffin explains the inherent limitations of single-vector search, such as information loss from pooling and poor out-of-domain performance, and introduces late interaction (multi-vector) models as a superior solution. He demonstrates how these models excel in long-context and reasoning-intensive tasks and presents the PyLate library to make training and evaluating these powerful models more accessible.\n\n00:00 - Introduction\n00:32 - About Me\n01:40 - Dense (Single) Vector Search Explained\n03:08 - Single Vector Models: The Go-To for RAG\n03:55 - Performance Evaluation & MTEB Leaderboard\n04:17 - The BEIR Benchmark & Goodhart's Law\n05:36 - Limitations Beyond Benchmarks: The Long Context Problem\n06:33 - Limitations Beyond Benchmarks: Reasoning-Intensive Retrieval\n07:50 - The Role of BM25\n08:24 - Pooling: The Intrinsic Flaw of Dense Models\n11:32 - Replacing Pooling with Late Interaction\n12:17 - Why Not Just Use a Bigger Single Vector?\n13:51 - Late Interaction: A Simple, Yet Effective, Difference\n16:48 - Interpretability: A Nice Little Bonus\n17:42 - Why Are People Still Using Dense Models?\n18:43 - PyLate: Extending Sentence Transformers for Multi-Vector Models\n21:28 - Training is Cool, Show Me the Evals\n22:49 - What Are the Future Avenues?\n24:36 - Conclusion & QR Codes\n25:52 - Q&A: Latency of Late Interaction vs. Dense Vector Models\n31:00 - Q&A: Does Fine-Tuning Close the Performance Gap?\n33:20 - Q&A: How Easy is it to Fine-Tune with PyLate?\n34:22 - Q&A: Common Mistakes When Moving from Single to Multi-Vector?",
    "crumbs": [
      "yt"
    ]
  },
  {
    "objectID": "yt.html#fetch-youtube-transcript",
    "href": "yt.html#fetch-youtube-transcript",
    "title": "yt",
    "section": "Fetch YouTube Transcript",
    "text": "Fetch YouTube Transcript\nFetch the youtube transcript from public videos.\n\nsource\n\ntranscribe\n\n transcribe (url, seconds_only=False)\n\nDownload YouTube transcript.\n\nt = transcribe(\"https://youtu.be/1x3k0V2IITo\")\n\n\nprint(t[:500])\n\n[00:00:00] Hello everyone, my name is Chapan and I\n[00:00:02] am a research engineer at Leighton and\n[00:00:05] today I will detail some of the limits\n[00:00:08] of single vector search that have been\n[00:00:10] highlighted by recent usages and\n[00:00:13] evaluations and then I will introduce\n[00:00:16] multi vector models also known as late\n[00:00:18] interaction models and how they can\n[00:00:21] overcome this and to finish I will\n[00:00:24] briefly present the pilot library that\n[00:00:26] al",
    "crumbs": [
      "yt"
    ]
  }
]